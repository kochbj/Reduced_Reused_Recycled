{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af1aaed-9b9a-4ea3-b52e-f2ea71345db1",
   "metadata": {},
   "source": [
    "# Dataset Construction\n",
    "\n",
    "This notebook constructs the datasets used in the analyses. To see the actual statistical analyses, see the R scripts.\n",
    "\n",
    "First load the PWC provided data. \"evaluation-tables.json\" and \"papers-with-abstracts.json\" were downloaded from [PWC's Github](https://github.com/paperswithcode/paperswithcode-data) on 06/16/2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "096cd869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'DATASET_PATH'\n",
      "/mnt/c/Users/berna/Documents/GitHub/Life_of_a_Benchmark/Dataset_Curation\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH=\"/mnt/c/Users/berna/Documents/GitHub/Life_of_a_Benchmark/Dataset_Curation\"\n",
    "%cd DATASET_PATH\n",
    "import pandas as pd\n",
    "import json\n",
    "with open('./PWC_Data/papers-with-abstracts.json') as f:\n",
    "    pwc=json.load(f)\n",
    "with open('./PWC_Data/evaluation-tables.json') as f:\n",
    "    benchmark_tables=json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba6cb7-1125-481b-8205-a1178e8e9ea1",
   "metadata": {},
   "source": [
    "### Construct Task Ontology\n",
    "\n",
    "This block parses \"evaluation-tables.json\" to create a task ontology. Benchmarks in this file are organized by task and subtask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5118e98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45513, 11)\n",
      "Number of papers used to construct task ontology:  (7041,)\n",
      "Number of tasks:  2002\n",
      "Mean number of parents/children for a task:  0.551948051948052\n",
      "Mean number of siblings for a task:  4.715784215784216\n"
     ]
    }
   ],
   "source": [
    "all_rows=[] #used to construct benchmark_papers df\n",
    "parent_child_dict={} #dictionary capturing parent-child task relations used to create task_relations df\n",
    "child_parent_dict={} #inverse of above\n",
    "task_category_dict={} #captures task category relations: categories in PWC are larger domains like \"NLP\",\"CV\",\"Methodology\n",
    "paper_titles_tasks={} #paper titles to tasks\n",
    "paper_titles_parent_tasks={} #paper totiles to parent tasks\n",
    "dataset_associated_tasks={} # tasks associated with the dataset\n",
    "\n",
    "for i, task in enumerate(benchmark_tables):\n",
    "    task_dict={} # A dictionary for each task that will ultimately be a row in the benchmark_papers dataframe\n",
    "    task_dict['task']=task['task']\n",
    "    task_dict['task_categories']=task['categories']\n",
    "    task_dict['task_description']=task['description']\n",
    "    task_dict['parent_task']=task['task']\n",
    "    if task['task'] not in parent_child_dict: parent_child_dict[task['task']]=[]\n",
    "    if task['task'] not in child_parent_dict: child_parent_dict[task['task']]=[]\n",
    "    task_category_dict[task['task']]=task['categories']\n",
    "    if len(task['datasets'])!=0: #if there are datasets associated with task\n",
    "        for j,d in enumerate(task['datasets']):\n",
    "            if d['dataset'] not in dataset_associated_tasks:dataset_associated_tasks[d['dataset']]=[]\n",
    "            parent_dataset=d['dataset'] # variations of datasets can be listed as \"children\"\n",
    "            dataset_associated_tasks[d['dataset']].append(task['task'])\n",
    "            dataset_dict={}\n",
    "            dataset_dict.update(task_dict)\n",
    "            dataset_dict['dataset']=d['dataset']\n",
    "            dataset_dict['dataset_citations']=d['dataset_citations']\n",
    "            dataset_dict['dataset_links']=d['dataset_links']\n",
    "            dataset_dict['dataset_subdatasets']=d['subdatasets']\n",
    "            dataset_dict['task']=task['task']\n",
    "            for row in d['sota']['rows']:\n",
    "                for m in row['metrics']:\n",
    "                    row_dict=dict(row)\n",
    "                    row_dict['metrics']=m\n",
    "                    row_dict['score']=row['metrics'][m]\n",
    "                    row_dict.update(dataset_dict)\n",
    "                    all_rows.append(row_dict)\n",
    "                if row['paper_title'] not in paper_titles_tasks:paper_titles_tasks[row['paper_title']]=[]\n",
    "                if row['paper_title'] not in paper_titles_parent_tasks:paper_titles_parent_tasks[row['paper_title']]=[]\n",
    "                paper_titles_tasks[row['paper_title']]+=[task['task']]\n",
    "                paper_titles_parent_tasks[row['paper_title']]+=[task['task']]\n",
    "    if len(task['subtasks'])!=0: #tasks can have subtasks. This is not tree. A subtask could have multiple parents or be a parent itself.\n",
    "        for t in task['subtasks']:\n",
    "            task_dict={} #Each subtask is it's own row also\n",
    "            if t['task'] not in child_parent_dict: child_parent_dict[t['task']]=[]\n",
    "            if t['task'] not in parent_child_dict: parent_child_dict[t['task']]=[]\n",
    "            task_category_dict[t['task']]=t['categories']\n",
    "            parent_child_dict[task['task']].append(t['task'])\n",
    "            child_parent_dict[t['task']].append(task['task'])\n",
    "            \n",
    "            task_dict['parent_task']=task['task']\n",
    "            task_dict['task']=t['task']\n",
    "            task_dict['task_categories']=','.join(t['categories'])\n",
    "            task_dict['task_description']=t['description']\n",
    "            \n",
    "            if len(t['datasets'])!=0:\n",
    "                for d in t['datasets']:\n",
    "                    if d['dataset'] not in dataset_associated_tasks:dataset_associated_tasks[d['dataset']]=[]\n",
    "                    dataset_associated_tasks[d['dataset']].append(t['task'])\n",
    "                    dataset_associated_tasks[parent_dataset].append(t['task'])\n",
    "                    dataset_dict={}\n",
    "                    dataset_dict.update(task_dict)\n",
    "                    dataset_dict['dataset']=d['dataset']\n",
    "                    dataset_dict['dataset_citations']=d['dataset_citations']\n",
    "                    dataset_dict['dataset_links']=d['dataset_links']\n",
    "                    dataset_dict['dataset_subdatasets']=d['subdatasets']\n",
    "                    dataset_dict['task']=t['task']\n",
    "                    for row in d['sota']['rows']:\n",
    "                        for m in row['metrics']:\n",
    "                            row_dict=dict(row)\n",
    "                            row_dict['metrics']=m\n",
    "                            row_dict['score']=row['metrics'][m]\n",
    "                            row_dict.update(dataset_dict)\n",
    "                            all_rows.append(row_dict)\n",
    "                        if row['paper_title'] not in paper_titles_tasks:paper_titles_tasks[row['paper_title']]=[]\n",
    "                        paper_titles_tasks[row['paper_title']]+=[t['task']] \n",
    "                        if row['paper_title'] not in paper_titles_parent_tasks:paper_titles_parent_tasks[row['paper_title']]=[]\n",
    "                        paper_titles_parent_tasks[row['paper_title']]+=[task['task']] \n",
    "\n",
    "#contains all relevant info at papers used in benchmarks\n",
    "benchmark_papers=pd.DataFrame(all_rows)\n",
    "benchmark_papers=benchmark_papers.drop_duplicates(['model_name','dataset','task','metrics','score'])\n",
    "benchmark_papers['benchmark_id']=benchmark_papers.groupby(['task','dataset','metrics']).ngroup()\n",
    "benchmark_papers=benchmark_papers[['paper_title','paper_date','task','parent_task','dataset','score','benchmark_id','metrics','task_categories']]\n",
    "benchmark_papers=benchmark_papers.drop_duplicates(['paper_title','paper_date','task','parent_task','dataset','score','benchmark_id','metrics'])\n",
    "benchmark_papers['CV']=benchmark_papers.apply(lambda row: 'Computer Vision' in row['task_categories'] and len(row['task_categories'])==1,axis=1 )\n",
    "benchmark_papers['NLP']=benchmark_papers.apply(lambda row: 'Natural Language Processing' in row['task_categories'] and len(row['task_categories'])==1,axis=1 )\n",
    "print(benchmark_papers.shape)\n",
    "\n",
    "#constructs sibling relationships from all tasks,\n",
    "#Note that because it is not a tree, sibling relations do not correspond exactly to parent child\n",
    "sibling_dict={i:[] for i in child_parent_dict.keys()}\n",
    "for k in sibling_dict.keys():\n",
    "    parents=child_parent_dict[k]\n",
    "    for i in parents:\n",
    "        sibling_dict[k]+=parent_child_dict[i]\n",
    "    sibling_dict[k]=list(set(sibling_dict[k]))\n",
    "\n",
    "# Create the ontology of task relations\n",
    "task_relations=pd.DataFrame({'categories':task_category_dict,'parents':child_parent_dict,'children':parent_child_dict,'siblings':sibling_dict})\n",
    "task_relations.index=task_relations.index.rename('task')\n",
    "task_relations=task_relations.reset_index()\n",
    "\n",
    "#Create a df labeling papers with tasks and parent tasks\n",
    "paper_relations=pd.DataFrame({'all_tasks':paper_titles_tasks,'all_parent_tasks':paper_titles_parent_tasks})\n",
    "paper_relations.index=paper_relations.index.rename('title')\n",
    "paper_relations=paper_relations.reset_index()\n",
    "task_relations.to_csv('task_relations.tsv',sep='\\t',quoting=1)\n",
    "\n",
    "print(\"Number of papers used to construct task ontology: \",benchmark_papers['paper_title'].drop_duplicates().shape)\n",
    "print(\"Number of tasks: \", task_relations.shape[0])\n",
    "print(\"Mean number of parents/children for a task: \", task_relations['parents'].apply(lambda x: len(x)).mean())\n",
    "print(\"Mean number of siblings for a task: \", task_relations['siblings'].apply(lambda x: len(x)).mean())\n",
    "#At least in the benchmark relations, it appears that PWC does have a tree like structure\n",
    "benchmark_papers.to_json('./PWC_Data/Derivative_Datasets/benchmarks_with_datasets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d3947-fc01-4067-b668-3b0ada681327",
   "metadata": {},
   "source": [
    "This block goes through papers with abstracts, constructs a DF with unique task-papers (NOT unique papers) and annotates task relations from previous block.\n",
    "Furthermore, we add our additional manual annotations for papers that introduced datasets and were either not in papers-with-abstracts.json or had no tasks labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "adbb0f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of manually annotated dataset-introducing papers:  90\n",
      "TASK NOT FOUND Text Segmentation (THIS TASK WAS NEVER USED IN A BENCHMARK)\n",
      "TASK NOT FOUND Audio Classificiation (THIS TASK WAS NEVER USED IN A BENCHMARK)\n",
      "TASK NOT FOUND 3D Object Modeling (THIS TASK WAS NEVER USED IN A BENCHMARK)\n",
      "TASK NOT FOUND Video Object Tracking? (THIS TASK WAS NEVER USED IN A BENCHMARK)\n",
      "TASK NOT FOUND Reasoning (THIS TASK WAS NEVER USED IN A BENCHMARK)\n",
      "Total PWC papers in papers with abstracts that have tasks:  (137554,)\n",
      "PWC papers lost because none of it's tasks are in the benchmarks dataset:  0\n"
     ]
    }
   ],
   "source": [
    "#These are all columns in the df for each paper\n",
    "title=[]\n",
    "pdf_url=[]\n",
    "paper_url=[]\n",
    "date=[]\n",
    "task=[]\n",
    "all_tasks=[]\n",
    "all_parents=[]\n",
    "all_children=[]\n",
    "all_categories=[]\n",
    "all_siblings=[]\n",
    "\n",
    "#These are used later but they note the number of papers per task and the first appearance of a paper with a task\n",
    "task_hist={}\n",
    "task_age={}\n",
    "\n",
    "\n",
    "#These are the datset-introducing papers that two authors manually annotated.\n",
    "#Note that only papers with the \"Justification\" column are those we both reviewed. \n",
    "sheet_id = '1Y3DDI6ySi9A6l3ZMET29EWSxBr8Uw-kvKn8RF2zYpzQ'\n",
    "sheet_name = 'untasked_datasets'\n",
    "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "manual_task_labels=pd.read_csv(url)\n",
    "manual_tasks_not_labeled=manual_task_labels[manual_task_labels.Justification.isnull()]\n",
    "manual_task_labels=manual_task_labels[~manual_task_labels.Justification.isnull()]\n",
    "print(\"Number of manually annotated dataset-introducing papers: \",manual_task_labels.shape[0])\n",
    "\n",
    "for i in pwc:\n",
    "    if i['title'] in list(manual_task_labels.title.str.strip()):\n",
    "        proposed_tasks=manual_task_labels[manual_task_labels.title==i['title']]['Proposed Tasks'].iloc[0]\n",
    "        #in addition to any tasks that were already there (although shouldn't be any), add the manually annotated ones\n",
    "        i['tasks']+=[j.strip() for j in proposed_tasks.split(',')]\n",
    "    if len(i['tasks'])==0: continue\n",
    "    #these comprehensions pool all parents,children, and children for ALL tasks the paper is labeled with\n",
    "    ap=[]\n",
    "    [ ap.extend(child_parent_dict[t]) for t in i['tasks'] if t in child_parent_dict]\n",
    "    ac =[]\n",
    "    [ ac.extend(task_category_dict[t]) for t in i['tasks'] if t in task_category_dict]\n",
    "    ah=[]\n",
    "    [ ah.extend(parent_child_dict[t]) for t in i['tasks'] if t in parent_child_dict]\n",
    "    asib=[]\n",
    "    [ asib.extend(sibling_dict[t]) for t in i['tasks'] if t in sibling_dict]\n",
    "\n",
    "    for t in i['tasks']:\n",
    "        #just keep up with task ages and counts\n",
    "        if t not in task_hist: task_hist[t]=0\n",
    "        task_hist[t]+=1\n",
    "        if t not in task_age: task_age[t] = pd.to_datetime(i['date']).year\n",
    "        else: task_age[t]= min(task_age[t],pd.to_datetime(i['date']).year)\n",
    "        \n",
    "        #add a row for each task-paper\n",
    "        title.append(i['title'])\n",
    "        pdf_url.append(i['url_pdf'])\n",
    "        paper_url.append(i['paper_url'])\n",
    "        date.append(i['date'])\n",
    "        task.append(t)\n",
    "        all_tasks.append(i['tasks'])\n",
    "        all_parents.append(list(set(ap)))\n",
    "        all_categories.append(list(set(ac)))\n",
    "        all_children.append(list(set(ah)))\n",
    "        all_siblings.append(list(set(asib)))\n",
    "\n",
    "#There are some mannually annotated dataset introducing papers that are not in papers-with-abstracts.json\n",
    "for i,row in manual_task_labels.iterrows():\n",
    "    if row['title'] in title: continue\n",
    "    #print(row['title'])\n",
    "    if type(row['Proposed Tasks'])==float:continue\n",
    "    tasks=[j.strip() for j in row['Proposed Tasks'].split(',')]\n",
    "    ap=[]\n",
    "    [ ap.extend(child_parent_dict[t]) for t in tasks if t in child_parent_dict]\n",
    "    ac =[]\n",
    "    [ ac.extend(task_category_dict[t]) for t in tasks if t in task_category_dict]\n",
    "    ah=[]\n",
    "    [ ah.extend(parent_child_dict[t]) for t in tasks if t in parent_child_dict]\n",
    "    asib=[]\n",
    "    [ asib.extend(sibling_dict[t]) for t in tasks if t in sibling_dict]\n",
    "    for t in tasks:\n",
    "        if t not in task_category_dict: print(\"TASK NOT FOUND\",t, \"(THIS TASK WAS NEVER USED IN A BENCHMARK)\")\n",
    "\n",
    "        if t not in task_hist: task_hist[t]=0\n",
    "        task_hist[t]+=1\n",
    "        if t not in task_age: task_age[t] = pd.to_datetime(row['introduced_date']).year\n",
    "        else: task_age[t]= min(task_age[t],pd.to_datetime(row['introduced_date']).year)\n",
    "        title.append(row['title'])\n",
    "        pdf_url.append(None)\n",
    "        paper_url.append(row['paper_url'])\n",
    "        date.append(row['introduced_date'])\n",
    "        task.append(t)\n",
    "        all_tasks.append(tasks)\n",
    "        all_parents.append(list(set(ap)))\n",
    "        all_categories.append(list(set(ac)))\n",
    "        all_children.append(list(set(ah)))\n",
    "        all_siblings.append(list(set(asib)))\n",
    "\n",
    "pwc_papers=pd.DataFrame({'title':title,'pdf_url':pdf_url,'paper_url':paper_url,'date':date,'task':task,\n",
    "                         'all_tasks':all_tasks,'all_parents':all_parents,'all_children':all_children,\n",
    "                         'all_siblings':all_siblings,'all_categories':all_categories,})\n",
    "\n",
    "print(\"Total PWC papers in papers with abstracts that have tasks: \",pwc_papers['title'].drop_duplicates().shape)\n",
    "task_hist=pd.Series(task_hist)\n",
    "#task_hist=task_hist[task_hist>task_hist.quantile(.3)]\n",
    "task_age=pd.Series(task_age)\n",
    "task_age.name='task_age'\n",
    "\n",
    "pwc_papers.to_json('./PWC_Data/Derivative_Datasets/pwc_papers.json')\n",
    "\n",
    "# Here we merge with task relations from the benchmarks\n",
    "pwc_papers_task=pd.merge(task_relations,pwc_papers,on='task')\n",
    "print(\"PWC papers lost because none of it's tasks are in the benchmarks dataset: \",\n",
    "      pwc_papers_task['title'].drop_duplicates().shape[0]-pwc_papers_task['title'].drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "51a13c8a-80b9-401e-a0ba-038477e65c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>categories</th>\n",
       "      <th>parents</th>\n",
       "      <th>children</th>\n",
       "      <th>siblings</th>\n",
       "      <th>title</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>date</th>\n",
       "      <th>all_tasks</th>\n",
       "      <th>all_parents</th>\n",
       "      <th>all_children</th>\n",
       "      <th>all_siblings</th>\n",
       "      <th>all_categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104206</th>\n",
       "      <td>Object Localization</td>\n",
       "      <td>[Computer Vision]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Weakly-Supervised Object Localization, Image-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>ImageNet: A large-scale hierarchical image dat...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://doi.org/10.1109/CVPR.2009.5206848</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>[Object Recognition, Image Classification, Obj...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Continuous Object Recognition, Artistic style...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Methodology, Computer Vision]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194776</th>\n",
       "      <td>Image Classification</td>\n",
       "      <td>[Computer Vision, Methodology]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Few-Shot Image Classification, Fine-Grained I...</td>\n",
       "      <td>[]</td>\n",
       "      <td>ImageNet: A large-scale hierarchical image dat...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://doi.org/10.1109/CVPR.2009.5206848</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>[Object Recognition, Image Classification, Obj...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Continuous Object Recognition, Artistic style...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Methodology, Computer Vision]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223006</th>\n",
       "      <td>Object Recognition</td>\n",
       "      <td>[Computer Vision]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[3D Object Recognition, Continuous Object Reco...</td>\n",
       "      <td>[]</td>\n",
       "      <td>ImageNet: A large-scale hierarchical image dat...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://doi.org/10.1109/CVPR.2009.5206848</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>[Object Recognition, Image Classification, Obj...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Continuous Object Recognition, Artistic style...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Methodology, Computer Vision]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        task                      categories parents  \\\n",
       "104206   Object Localization               [Computer Vision]      []   \n",
       "194776  Image Classification  [Computer Vision, Methodology]      []   \n",
       "223006    Object Recognition               [Computer Vision]      []   \n",
       "\n",
       "                                                 children siblings  \\\n",
       "104206  [Weakly-Supervised Object Localization, Image-...       []   \n",
       "194776  [Few-Shot Image Classification, Fine-Grained I...       []   \n",
       "223006  [3D Object Recognition, Continuous Object Reco...       []   \n",
       "\n",
       "                                                    title pdf_url  \\\n",
       "104206  ImageNet: A large-scale hierarchical image dat...    None   \n",
       "194776  ImageNet: A large-scale hierarchical image dat...    None   \n",
       "223006  ImageNet: A large-scale hierarchical image dat...    None   \n",
       "\n",
       "                                        paper_url        date  \\\n",
       "104206  https://doi.org/10.1109/CVPR.2009.5206848  2009-01-01   \n",
       "194776  https://doi.org/10.1109/CVPR.2009.5206848  2009-01-01   \n",
       "223006  https://doi.org/10.1109/CVPR.2009.5206848  2009-01-01   \n",
       "\n",
       "                                                all_tasks all_parents  \\\n",
       "104206  [Object Recognition, Image Classification, Obj...          []   \n",
       "194776  [Object Recognition, Image Classification, Obj...          []   \n",
       "223006  [Object Recognition, Image Classification, Obj...          []   \n",
       "\n",
       "                                             all_children all_siblings  \\\n",
       "104206  [Continuous Object Recognition, Artistic style...           []   \n",
       "194776  [Continuous Object Recognition, Artistic style...           []   \n",
       "223006  [Continuous Object Recognition, Artistic style...           []   \n",
       "\n",
       "                        all_categories  \n",
       "104206  [Methodology, Computer Vision]  \n",
       "194776  [Methodology, Computer Vision]  \n",
       "223006  [Methodology, Computer Vision]  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwc_papers_task[pwc_papers_task.title=='ImageNet: A large-scale hierarchical image database']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b4a3e-dc79-41d3-9d30-7785afd470ec",
   "metadata": {},
   "source": [
    "Third step is to load and clean the datasets from the PWC file datasets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6a29c596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-116-48e1091d225f>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  manual_dict['Proposed Tasks']=manual_dict['Proposed Tasks'].apply(lambda x: [j.strip() for j in x.split(',')] if type(x)!=float else [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets in PWC:  4384\n",
      "Datasets affiliated with a paper in PWC:  2673\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#This is available from PWC\n",
    "with open('/home/bkoch/Projects/DataProject/analyses/04-PWC/PWC_2021_06_16/datasets.json') as f:\n",
    "    datasets=pd.DataFrame(json.load(f))\n",
    "\n",
    "#Basic cleaning\n",
    "datasets['title']=datasets['paper'].apply(lambda js: js['title'] if js is not None else None)\n",
    "datasets['paper_url']=datasets['paper'].apply(lambda js: js['url'] if js is not None else None)\n",
    "datasets['introduced_date']=pd.to_datetime(datasets['introduced_date'])\n",
    "datasets['Texts']=datasets['modalities'].apply(lambda r: 'Texts' in r)\n",
    "datasets['Images']=datasets['modalities'].apply(lambda r: 'Images' in r)\n",
    "datasets['dataset_tasks']=datasets['tasks'].apply(lambda js: [ j['task'] for j in js])\n",
    "\n",
    "#Again add tasks from manually labeled dataset-papers\n",
    "manual_dict=manual_task_labels[['name','Proposed Tasks']]\n",
    "manual_dict['Proposed Tasks']=manual_dict['Proposed Tasks'].apply(lambda x: [j.strip() for j in x.split(',')] if type(x)!=float else [])\n",
    "manual_dict=manual_dict.set_index('name').to_dict()['Proposed Tasks']\n",
    "datasets['dataset_tasks']=datasets.apply(lambda row: row['dataset_tasks']+manual_dict[row['name']] if row['name'] in manual_dict else row['dataset_tasks'],axis=1)\n",
    "datasets=datasets.drop(['tasks','paper'],axis=1)\n",
    "\n",
    "#add all task relations to datasets\n",
    "all_parents=[]\n",
    "all_children=[]\n",
    "all_categories=[]\n",
    "all_siblings=[]\n",
    "for i,row in datasets.iterrows():\n",
    "    ap=[]\n",
    "    [ ap.extend(child_parent_dict[t]) for t in row['dataset_tasks'] if t in child_parent_dict]\n",
    "    ac =[]\n",
    "    [ ac.extend(task_category_dict[t]) for t in row['dataset_tasks'] if t in task_category_dict]\n",
    "    ah=[]\n",
    "    [ ah.extend(parent_child_dict[t]) for t in row['dataset_tasks'] if t in parent_child_dict]\n",
    "    asib=[]\n",
    "    [ asib.extend(sibling_dict[t]) for t in row['dataset_tasks'] if t in sibling_dict]\n",
    "    all_parents.append(ap)\n",
    "    all_categories.append(ac)\n",
    "    all_children.append(ah)\n",
    "    all_siblings.append(asib)\n",
    "datasets['dataset_tasks_parents']=all_parents\n",
    "datasets['dataset_tasks_categories']=all_categories\n",
    "datasets['dataset_tasks_children']=all_children\n",
    "datasets['dataset_tasks_siblings']=all_siblings\n",
    "datasets.to_json('./dataset_with_tasks.json')\n",
    "print(\"Total number of datasets in PWC: \",datasets['name'].drop_duplicates().shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# This is one dataset I noticed that is totally messed up with algorithmic task labels. Going to drop it.\n",
    "datasets=datasets[datasets.name!='PRID2011']\n",
    "\n",
    "datasets_pwc_total=pd.merge(datasets.drop('paper_url',axis=1),pwc_papers_task.drop('paper_url',axis=1),on=['title'],how='left')\n",
    "#datasets_pwc_total=pd.merge(datasets,pwc_papers_task,on=['title','paper_url'],how='left')\n",
    "#datasets_pwc_total.to_json('./DatawithTasks/datasets_total.json')\n",
    "\n",
    "datasets_pwc=pd.merge(datasets.drop('paper_url',axis=1),pwc_papers_task.drop('paper_url',axis=1),on=['title'])\n",
    "#datasets_pwc['all_tasks']=datasets_pwc.apply(lambda row: list(set(row['all_tasks']+row['dataset_tasks'])) if row['name'] in manual_dict else row['all_tasks'],axis=1)\n",
    "\n",
    "\n",
    "print(\"Datasets affiliated with a paper in PWC: \",datasets_pwc['name'].drop_duplicates().shape[0])\n",
    "datasets_pwc.to_json('./PWC_Data/Derivative_Datasets/datasets_pwc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6e62136a-4194-41d1-9bfc-7aee3c0cd305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>homepage</th>\n",
       "      <th>description</th>\n",
       "      <th>introduced_date</th>\n",
       "      <th>warning</th>\n",
       "      <th>modalities</th>\n",
       "      <th>languages</th>\n",
       "      <th>variants</th>\n",
       "      <th>...</th>\n",
       "      <th>parents</th>\n",
       "      <th>children</th>\n",
       "      <th>siblings</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>date</th>\n",
       "      <th>all_tasks</th>\n",
       "      <th>all_parents</th>\n",
       "      <th>all_children</th>\n",
       "      <th>all_siblings</th>\n",
       "      <th>all_categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://paperswithcode.com/dataset/mnist</td>\n",
       "      <td>MNIST</td>\n",
       "      <td></td>\n",
       "      <td>http://yann.lecun.com/exdb/mnist/</td>\n",
       "      <td>The **MNIST** database (**Modified National In...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>[Images]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[75 Superpixel MNIST, MNIST, MNIST-full, MNIST...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Active Learning, Handwriting Recognition, Han...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Handwriting Recognition, Optical Character Re...</td>\n",
       "      <td>[Optical Character Recognition]</td>\n",
       "      <td>[Irregular Text Recognition, Artistic style cl...</td>\n",
       "      <td>[Word Spotting In Handwritten Documents, Irreg...</td>\n",
       "      <td>[Methodology, Natural Language Processing, Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://paperswithcode.com/dataset/mnist</td>\n",
       "      <td>MNIST</td>\n",
       "      <td></td>\n",
       "      <td>http://yann.lecun.com/exdb/mnist/</td>\n",
       "      <td>The **MNIST** database (**Modified National In...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>[Images]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[75 Superpixel MNIST, MNIST, MNIST-full, MNIST...</td>\n",
       "      <td>...</td>\n",
       "      <td>[Optical Character Recognition]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Word Spotting In Handwritten Documents, Irreg...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Handwriting Recognition, Optical Character Re...</td>\n",
       "      <td>[Optical Character Recognition]</td>\n",
       "      <td>[Irregular Text Recognition, Artistic style cl...</td>\n",
       "      <td>[Word Spotting In Handwritten Documents, Irreg...</td>\n",
       "      <td>[Methodology, Natural Language Processing, Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://paperswithcode.com/dataset/mnist</td>\n",
       "      <td>MNIST</td>\n",
       "      <td></td>\n",
       "      <td>http://yann.lecun.com/exdb/mnist/</td>\n",
       "      <td>The **MNIST** database (**Modified National In...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>[Images]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[75 Superpixel MNIST, MNIST, MNIST-full, MNIST...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Few-Shot Image Classification, Fine-Grained I...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Handwriting Recognition, Optical Character Re...</td>\n",
       "      <td>[Optical Character Recognition]</td>\n",
       "      <td>[Irregular Text Recognition, Artistic style cl...</td>\n",
       "      <td>[Word Spotting In Handwritten Documents, Irreg...</td>\n",
       "      <td>[Methodology, Natural Language Processing, Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://paperswithcode.com/dataset/celeba</td>\n",
       "      <td>CelebA</td>\n",
       "      <td>CelebFaces Attributes Dataset</td>\n",
       "      <td>http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</td>\n",
       "      <td>CelebFaces Attributes dataset contains 202,599...</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>[Images]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[CelebA Aligned, CelebA 64x64, CelebA 256x256,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[Facial Recognition and Modelling]</td>\n",
       "      <td>[Occluded Face Detection]</td>\n",
       "      <td>[Face Image Retrieval, Facial Attribute Classi...</td>\n",
       "      <td>https://arxiv.org/pdf/1411.7766v3.pdf</td>\n",
       "      <td>2014-11-28</td>\n",
       "      <td>[Face Detection, Facial Attribute Classification]</td>\n",
       "      <td>[Facial Recognition and Modelling]</td>\n",
       "      <td>[Occluded Face Detection]</td>\n",
       "      <td>[Face Image Retrieval, Facial Attribute Classi...</td>\n",
       "      <td>[Computer Vision]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://paperswithcode.com/dataset/celeba</td>\n",
       "      <td>CelebA</td>\n",
       "      <td>CelebFaces Attributes Dataset</td>\n",
       "      <td>http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</td>\n",
       "      <td>CelebFaces Attributes dataset contains 202,599...</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>[Images]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[CelebA Aligned, CelebA 64x64, CelebA 256x256,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[Facial Recognition and Modelling]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Face Image Retrieval, Facial Attribute Classi...</td>\n",
       "      <td>https://arxiv.org/pdf/1411.7766v3.pdf</td>\n",
       "      <td>2014-11-28</td>\n",
       "      <td>[Face Detection, Facial Attribute Classification]</td>\n",
       "      <td>[Facial Recognition and Modelling]</td>\n",
       "      <td>[Occluded Face Detection]</td>\n",
       "      <td>[Face Image Retrieval, Facial Attribute Classi...</td>\n",
       "      <td>[Computer Vision]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5796</th>\n",
       "      <td>https://paperswithcode.com/dataset/fewclue</td>\n",
       "      <td>FewCLUE</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/CLUEbenchmark/FewCLUE</td>\n",
       "      <td>Chinese Few-shot Learning Evaluation Benchmark...</td>\n",
       "      <td>2021-07-15</td>\n",
       "      <td>None</td>\n",
       "      <td>[Texts]</td>\n",
       "      <td>[Chinese]</td>\n",
       "      <td>[FewCLUE]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Temporal Action Localization, Generalized Zer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://arxiv.org/pdf/2107.07498v1.pdf</td>\n",
       "      <td>2021-07-15</td>\n",
       "      <td>[Few-Shot Learning, Machine Reading Comprehens...</td>\n",
       "      <td>[Visual Question Answering, Reading Comprehens...</td>\n",
       "      <td>[Few-Shot Relation Classification, Machine Rea...</td>\n",
       "      <td>[Machine Reading Comprehension, Few-shot Regre...</td>\n",
       "      <td>[Methodology, Natural Language Processing, Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5797</th>\n",
       "      <td>https://paperswithcode.com/dataset/fewclue</td>\n",
       "      <td>FewCLUE</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/CLUEbenchmark/FewCLUE</td>\n",
       "      <td>Chinese Few-shot Learning Evaluation Benchmark...</td>\n",
       "      <td>2021-07-15</td>\n",
       "      <td>None</td>\n",
       "      <td>[Texts]</td>\n",
       "      <td>[Chinese]</td>\n",
       "      <td>[FewCLUE]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://arxiv.org/pdf/2107.07498v1.pdf</td>\n",
       "      <td>2021-07-15</td>\n",
       "      <td>[Few-Shot Learning, Machine Reading Comprehens...</td>\n",
       "      <td>[Visual Question Answering, Reading Comprehens...</td>\n",
       "      <td>[Few-Shot Relation Classification, Machine Rea...</td>\n",
       "      <td>[Machine Reading Comprehension, Few-shot Regre...</td>\n",
       "      <td>[Methodology, Natural Language Processing, Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5798</th>\n",
       "      <td>https://paperswithcode.com/dataset/zs-f-vqa</td>\n",
       "      <td>ZS-F-VQA</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/China-UK-ZSL/ZS-F-VQA</td>\n",
       "      <td>The ZS-F-VQA dataset  is a new split of the F-...</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>None</td>\n",
       "      <td>[Images, Texts, Graphs]</td>\n",
       "      <td>[English]</td>\n",
       "      <td>[ZS-F-VQA]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Open-Domain Question Answering, Answer Select...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://arxiv.org/pdf/2107.05348v3.pdf</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>[Knowledge Graphs, Question Answering, Visual ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Answer Selection, Logical Reasoning Question ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Natural Language Processing, Knowledge Base, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5799</th>\n",
       "      <td>https://paperswithcode.com/dataset/zs-f-vqa</td>\n",
       "      <td>ZS-F-VQA</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/China-UK-ZSL/ZS-F-VQA</td>\n",
       "      <td>The ZS-F-VQA dataset  is a new split of the F-...</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>None</td>\n",
       "      <td>[Images, Texts, Graphs]</td>\n",
       "      <td>[English]</td>\n",
       "      <td>[ZS-F-VQA]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Machine Reading Comprehension, Embodied Quest...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://arxiv.org/pdf/2107.05348v3.pdf</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>[Knowledge Graphs, Question Answering, Visual ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Answer Selection, Logical Reasoning Question ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Natural Language Processing, Knowledge Base, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800</th>\n",
       "      <td>https://paperswithcode.com/dataset/zs-f-vqa</td>\n",
       "      <td>ZS-F-VQA</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/China-UK-ZSL/ZS-F-VQA</td>\n",
       "      <td>The ZS-F-VQA dataset  is a new split of the F-...</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>None</td>\n",
       "      <td>[Images, Texts, Graphs]</td>\n",
       "      <td>[English]</td>\n",
       "      <td>[ZS-F-VQA]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Knowledge Graph Completion, Open Knowledge Gr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://arxiv.org/pdf/2107.05348v3.pdf</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>[Knowledge Graphs, Question Answering, Visual ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Answer Selection, Logical Reasoning Question ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Natural Language Processing, Knowledge Base, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5801 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              url      name  \\\n",
       "0        https://paperswithcode.com/dataset/mnist     MNIST   \n",
       "1        https://paperswithcode.com/dataset/mnist     MNIST   \n",
       "2        https://paperswithcode.com/dataset/mnist     MNIST   \n",
       "3       https://paperswithcode.com/dataset/celeba    CelebA   \n",
       "4       https://paperswithcode.com/dataset/celeba    CelebA   \n",
       "...                                           ...       ...   \n",
       "5796   https://paperswithcode.com/dataset/fewclue   FewCLUE   \n",
       "5797   https://paperswithcode.com/dataset/fewclue   FewCLUE   \n",
       "5798  https://paperswithcode.com/dataset/zs-f-vqa  ZS-F-VQA   \n",
       "5799  https://paperswithcode.com/dataset/zs-f-vqa  ZS-F-VQA   \n",
       "5800  https://paperswithcode.com/dataset/zs-f-vqa  ZS-F-VQA   \n",
       "\n",
       "                          full_name  \\\n",
       "0                                     \n",
       "1                                     \n",
       "2                                     \n",
       "3     CelebFaces Attributes Dataset   \n",
       "4     CelebFaces Attributes Dataset   \n",
       "...                             ...   \n",
       "5796                                  \n",
       "5797                                  \n",
       "5798                                  \n",
       "5799                                  \n",
       "5800                                  \n",
       "\n",
       "                                              homepage  \\\n",
       "0                    http://yann.lecun.com/exdb/mnist/   \n",
       "1                    http://yann.lecun.com/exdb/mnist/   \n",
       "2                    http://yann.lecun.com/exdb/mnist/   \n",
       "3     http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html   \n",
       "4     http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html   \n",
       "...                                                ...   \n",
       "5796          https://github.com/CLUEbenchmark/FewCLUE   \n",
       "5797          https://github.com/CLUEbenchmark/FewCLUE   \n",
       "5798          https://github.com/China-UK-ZSL/ZS-F-VQA   \n",
       "5799          https://github.com/China-UK-ZSL/ZS-F-VQA   \n",
       "5800          https://github.com/China-UK-ZSL/ZS-F-VQA   \n",
       "\n",
       "                                            description introduced_date  \\\n",
       "0     The **MNIST** database (**Modified National In...             NaT   \n",
       "1     The **MNIST** database (**Modified National In...             NaT   \n",
       "2     The **MNIST** database (**Modified National In...             NaT   \n",
       "3     CelebFaces Attributes dataset contains 202,599...      2015-01-01   \n",
       "4     CelebFaces Attributes dataset contains 202,599...      2015-01-01   \n",
       "...                                                 ...             ...   \n",
       "5796  Chinese Few-shot Learning Evaluation Benchmark...      2021-07-15   \n",
       "5797  Chinese Few-shot Learning Evaluation Benchmark...      2021-07-15   \n",
       "5798  The ZS-F-VQA dataset  is a new split of the F-...      2021-07-12   \n",
       "5799  The ZS-F-VQA dataset  is a new split of the F-...      2021-07-12   \n",
       "5800  The ZS-F-VQA dataset  is a new split of the F-...      2021-07-12   \n",
       "\n",
       "     warning               modalities  languages  \\\n",
       "0       None                 [Images]         []   \n",
       "1       None                 [Images]         []   \n",
       "2       None                 [Images]         []   \n",
       "3       None                 [Images]         []   \n",
       "4       None                 [Images]         []   \n",
       "...      ...                      ...        ...   \n",
       "5796    None                  [Texts]  [Chinese]   \n",
       "5797    None                  [Texts]  [Chinese]   \n",
       "5798    None  [Images, Texts, Graphs]  [English]   \n",
       "5799    None  [Images, Texts, Graphs]  [English]   \n",
       "5800    None  [Images, Texts, Graphs]  [English]   \n",
       "\n",
       "                                               variants  ...  \\\n",
       "0     [75 Superpixel MNIST, MNIST, MNIST-full, MNIST...  ...   \n",
       "1     [75 Superpixel MNIST, MNIST, MNIST-full, MNIST...  ...   \n",
       "2     [75 Superpixel MNIST, MNIST, MNIST-full, MNIST...  ...   \n",
       "3     [CelebA Aligned, CelebA 64x64, CelebA 256x256,...  ...   \n",
       "4     [CelebA Aligned, CelebA 64x64, CelebA 256x256,...  ...   \n",
       "...                                                 ...  ...   \n",
       "5796                                          [FewCLUE]  ...   \n",
       "5797                                          [FewCLUE]  ...   \n",
       "5798                                         [ZS-F-VQA]  ...   \n",
       "5799                                         [ZS-F-VQA]  ...   \n",
       "5800                                         [ZS-F-VQA]  ...   \n",
       "\n",
       "                                 parents  \\\n",
       "0                                     []   \n",
       "1        [Optical Character Recognition]   \n",
       "2                                     []   \n",
       "3     [Facial Recognition and Modelling]   \n",
       "4     [Facial Recognition and Modelling]   \n",
       "...                                  ...   \n",
       "5796                                  []   \n",
       "5797                                  []   \n",
       "5798                                  []   \n",
       "5799                                  []   \n",
       "5800                                  []   \n",
       "\n",
       "                                               children  \\\n",
       "0     [Active Learning, Handwriting Recognition, Han...   \n",
       "1                                                    []   \n",
       "2     [Few-Shot Image Classification, Fine-Grained I...   \n",
       "3                             [Occluded Face Detection]   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "5796  [Temporal Action Localization, Generalized Zer...   \n",
       "5797                                                 []   \n",
       "5798  [Open-Domain Question Answering, Answer Select...   \n",
       "5799  [Machine Reading Comprehension, Embodied Quest...   \n",
       "5800  [Knowledge Graph Completion, Open Knowledge Gr...   \n",
       "\n",
       "                                               siblings  \\\n",
       "0                                                    []   \n",
       "1     [Word Spotting In Handwritten Documents, Irreg...   \n",
       "2                                                    []   \n",
       "3     [Face Image Retrieval, Facial Attribute Classi...   \n",
       "4     [Face Image Retrieval, Facial Attribute Classi...   \n",
       "...                                                 ...   \n",
       "5796                                                 []   \n",
       "5797                                                 []   \n",
       "5798                                                 []   \n",
       "5799                                                 []   \n",
       "5800                                                 []   \n",
       "\n",
       "                                     pdf_url        date  \\\n",
       "0                                       None         NaN   \n",
       "1                                       None         NaN   \n",
       "2                                       None         NaN   \n",
       "3      https://arxiv.org/pdf/1411.7766v3.pdf  2014-11-28   \n",
       "4      https://arxiv.org/pdf/1411.7766v3.pdf  2014-11-28   \n",
       "...                                      ...         ...   \n",
       "5796  https://arxiv.org/pdf/2107.07498v1.pdf  2021-07-15   \n",
       "5797  https://arxiv.org/pdf/2107.07498v1.pdf  2021-07-15   \n",
       "5798  https://arxiv.org/pdf/2107.05348v3.pdf  2021-07-12   \n",
       "5799  https://arxiv.org/pdf/2107.05348v3.pdf  2021-07-12   \n",
       "5800  https://arxiv.org/pdf/2107.05348v3.pdf  2021-07-12   \n",
       "\n",
       "                                              all_tasks  \\\n",
       "0     [Handwriting Recognition, Optical Character Re...   \n",
       "1     [Handwriting Recognition, Optical Character Re...   \n",
       "2     [Handwriting Recognition, Optical Character Re...   \n",
       "3     [Face Detection, Facial Attribute Classification]   \n",
       "4     [Face Detection, Facial Attribute Classification]   \n",
       "...                                                 ...   \n",
       "5796  [Few-Shot Learning, Machine Reading Comprehens...   \n",
       "5797  [Few-Shot Learning, Machine Reading Comprehens...   \n",
       "5798  [Knowledge Graphs, Question Answering, Visual ...   \n",
       "5799  [Knowledge Graphs, Question Answering, Visual ...   \n",
       "5800  [Knowledge Graphs, Question Answering, Visual ...   \n",
       "\n",
       "                                            all_parents  \\\n",
       "0                       [Optical Character Recognition]   \n",
       "1                       [Optical Character Recognition]   \n",
       "2                       [Optical Character Recognition]   \n",
       "3                    [Facial Recognition and Modelling]   \n",
       "4                    [Facial Recognition and Modelling]   \n",
       "...                                                 ...   \n",
       "5796  [Visual Question Answering, Reading Comprehens...   \n",
       "5797  [Visual Question Answering, Reading Comprehens...   \n",
       "5798                                                 []   \n",
       "5799                                                 []   \n",
       "5800                                                 []   \n",
       "\n",
       "                                           all_children  \\\n",
       "0     [Irregular Text Recognition, Artistic style cl...   \n",
       "1     [Irregular Text Recognition, Artistic style cl...   \n",
       "2     [Irregular Text Recognition, Artistic style cl...   \n",
       "3                             [Occluded Face Detection]   \n",
       "4                             [Occluded Face Detection]   \n",
       "...                                                 ...   \n",
       "5796  [Few-Shot Relation Classification, Machine Rea...   \n",
       "5797  [Few-Shot Relation Classification, Machine Rea...   \n",
       "5798  [Answer Selection, Logical Reasoning Question ...   \n",
       "5799  [Answer Selection, Logical Reasoning Question ...   \n",
       "5800  [Answer Selection, Logical Reasoning Question ...   \n",
       "\n",
       "                                           all_siblings  \\\n",
       "0     [Word Spotting In Handwritten Documents, Irreg...   \n",
       "1     [Word Spotting In Handwritten Documents, Irreg...   \n",
       "2     [Word Spotting In Handwritten Documents, Irreg...   \n",
       "3     [Face Image Retrieval, Facial Attribute Classi...   \n",
       "4     [Face Image Retrieval, Facial Attribute Classi...   \n",
       "...                                                 ...   \n",
       "5796  [Machine Reading Comprehension, Few-shot Regre...   \n",
       "5797  [Machine Reading Comprehension, Few-shot Regre...   \n",
       "5798                                                 []   \n",
       "5799                                                 []   \n",
       "5800                                                 []   \n",
       "\n",
       "                                         all_categories  \n",
       "0     [Methodology, Natural Language Processing, Com...  \n",
       "1     [Methodology, Natural Language Processing, Com...  \n",
       "2     [Methodology, Natural Language Processing, Com...  \n",
       "3                                     [Computer Vision]  \n",
       "4                                     [Computer Vision]  \n",
       "...                                                 ...  \n",
       "5796  [Methodology, Natural Language Processing, Com...  \n",
       "5797  [Methodology, Natural Language Processing, Com...  \n",
       "5798  [Natural Language Processing, Knowledge Base, ...  \n",
       "5799  [Natural Language Processing, Knowledge Base, ...  \n",
       "5800  [Natural Language Processing, Knowledge Base, ...  \n",
       "\n",
       "[5801 rows x 32 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_pwc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf363b5-5b01-4770-86d4-bfb38d8dfa68",
   "metadata": {},
   "source": [
    "4. Load dataset-citing papers. This scraped with an API in the script Get_Dataset_Citing_Papers.ipynb Here we again link to papers-with-abstracts.json in order to get task information. We lose a significant number of papers doing this, but it's hard to say whether these papers are real usages or just keyword hits anyways. It is possible that manual annotation could recover some of these papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ca21ae2f-dc82-4c7e-9a3f-6e44bfabd3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset citing papers harvested from PWC internal API: 60647\n",
      "Dataset citing papers that are actually labeled wth tasks:  46697\n"
     ]
    }
   ],
   "source": [
    "dataset_citing_papers=pd.read_csv('./PWC_Data/Derivative_Datasets/datasets_citing_papers.txt',sep='\\t')\n",
    "print(\"Dataset citing papers harvested from PWC internal API:\",dataset_citing_papers['title'].drop_duplicates().shape[0])\n",
    "\n",
    "dataset_citing_papers_pwc=pd.merge(dataset_citing_papers,pwc_papers_task,on=['title','date'])\n",
    "dataset_citing_papers_pwc['date']=pd.to_datetime(dataset_citing_papers_pwc['date'])\n",
    "print(\"Dataset citing papers that are actually labeled wth tasks: \",dataset_citing_papers_pwc['title'].drop_duplicates().shape[0])\n",
    "dataset_citing_papers_pwc.to_json('./PWC_Data/Derivative_Datasets/datasets_citing_papers_pwc.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "161f502f-206f-42f4-8b3b-fe6c97159892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of usages recovered through manual annotation: 33739\n",
      "Number of usages NOT recovered through manual annotation: 9546\n",
      "Percentage of total usages we're dropping:  0.10278441759803605\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of usages recovered through manual annotation:\", dataset_citing_papers_pwc[dataset_citing_papers_pwc.name.isin(manual_task_labels.name)].drop_duplicates(['name','title']).shape[0])\n",
    "print(\"Number of usages NOT recovered through manual annotation:\",dataset_citing_papers_pwc[dataset_citing_papers_pwc.name.isin(manual_tasks_not_labeled.name)].drop_duplicates(['name','title']).shape[0])\n",
    "#So this number goes up as we add more anlyses\n",
    "print(\"Percentage of total usages we're dropping: \",dataset_citing_papers_pwc[dataset_citing_papers_pwc.name.isin(manual_tasks_not_labeled.name)].drop_duplicates(['name','title']).shape[0]/(dataset_citing_papers_pwc.drop_duplicates(['name','title']).shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f8389008-5a51-4492-a9d3-01b92d58543a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pwc_dataset_id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>is_problematic</th>\n",
       "      <th>paper_count</th>\n",
       "      <th>task</th>\n",
       "      <th>categories</th>\n",
       "      <th>parents</th>\n",
       "      <th>children</th>\n",
       "      <th>siblings</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>all_tasks</th>\n",
       "      <th>all_parents</th>\n",
       "      <th>all_children</th>\n",
       "      <th>all_siblings</th>\n",
       "      <th>all_categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNIST</td>\n",
       "      <td>1</td>\n",
       "      <td>Modularity Matters: Learning Invariant Relatio...</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>False</td>\n",
       "      <td>4159</td>\n",
       "      <td>Relational Reasoning</td>\n",
       "      <td>[Natural Language Processing]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>http://arxiv.org/pdf/1806.06765v1.pdf</td>\n",
       "      <td>https://paperswithcode.com/paper/modularity-ma...</td>\n",
       "      <td>[Relational Reasoning, Visual Reasoning]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Visual Commonsense Reasoning]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Reasoning, Natural Language Processing, Compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SHAPES</td>\n",
       "      <td>3452</td>\n",
       "      <td>Modularity Matters: Learning Invariant Relatio...</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>False</td>\n",
       "      <td>67</td>\n",
       "      <td>Relational Reasoning</td>\n",
       "      <td>[Natural Language Processing]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>http://arxiv.org/pdf/1806.06765v1.pdf</td>\n",
       "      <td>https://paperswithcode.com/paper/modularity-ma...</td>\n",
       "      <td>[Relational Reasoning, Visual Reasoning]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Visual Commonsense Reasoning]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Reasoning, Natural Language Processing, Compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MNIST</td>\n",
       "      <td>1</td>\n",
       "      <td>HitNet: a neural network with capsules embedde...</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>False</td>\n",
       "      <td>4159</td>\n",
       "      <td>Data Augmentation</td>\n",
       "      <td>[Computer Vision, Natural Language Processing,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Image Augmentation, Text Augmentation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>http://arxiv.org/pdf/1806.06519v1.pdf</td>\n",
       "      <td>https://paperswithcode.com/paper/hitnet-a-neur...</td>\n",
       "      <td>[Data Augmentation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Image Augmentation, Text Augmentation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Methodology, Natural Language Processing, Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVHN</td>\n",
       "      <td>424</td>\n",
       "      <td>HitNet: a neural network with capsules embedde...</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>False</td>\n",
       "      <td>1606</td>\n",
       "      <td>Data Augmentation</td>\n",
       "      <td>[Computer Vision, Natural Language Processing,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Image Augmentation, Text Augmentation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>http://arxiv.org/pdf/1806.06519v1.pdf</td>\n",
       "      <td>https://paperswithcode.com/paper/hitnet-a-neur...</td>\n",
       "      <td>[Data Augmentation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Image Augmentation, Text Augmentation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Methodology, Natural Language Processing, Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>431</td>\n",
       "      <td>HitNet: a neural network with capsules embedde...</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>False</td>\n",
       "      <td>6230</td>\n",
       "      <td>Data Augmentation</td>\n",
       "      <td>[Computer Vision, Natural Language Processing,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Image Augmentation, Text Augmentation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>http://arxiv.org/pdf/1806.06519v1.pdf</td>\n",
       "      <td>https://paperswithcode.com/paper/hitnet-a-neur...</td>\n",
       "      <td>[Data Augmentation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Image Augmentation, Text Augmentation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Methodology, Natural Language Processing, Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217267</th>\n",
       "      <td>NucMM</td>\n",
       "      <td>8042</td>\n",
       "      <td>NucMM Dataset: 3D Neuronal Nuclei Instance Seg...</td>\n",
       "      <td>2021-07-13</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>[Computer Code, Time Series, Computer Vision]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Tumor Segmentation, 3D Semantic Segmentation,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://arxiv.org/pdf/2107.05840v1.pdf</td>\n",
       "      <td>https://paperswithcode.com/paper/nucmm-dataset...</td>\n",
       "      <td>[Electron Microscopy, Instance Segmentation, R...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Real-Time Semantic Segmentation, Referring Ex...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Methodology, Time Series, Computer Vision, Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217271</th>\n",
       "      <td>AxonEM</td>\n",
       "      <td>8043</td>\n",
       "      <td>AxonEM Dataset: 3D Axon Instance Segmentation ...</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>[Computer Code, Time Series, Computer Vision]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Tumor Segmentation, 3D Semantic Segmentation,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://arxiv.org/pdf/2107.05451v1.pdf</td>\n",
       "      <td>https://paperswithcode.com/paper/axonem-datase...</td>\n",
       "      <td>[Electron Microscopy, Instance Segmentation, S...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Real-Time Semantic Segmentation, Referring Ex...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Time Series, Computer Vision, Computer Code]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217274</th>\n",
       "      <td>MSJudge</td>\n",
       "      <td>8046</td>\n",
       "      <td>Legal Judgment Prediction with Multi-Stage Cas...</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Multi-Task Learning</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Transfer Learning]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Unsupervised Domain Expansion, Auxiliary Lear...</td>\n",
       "      <td>https://arxiv.org/pdf/2107.05192v1.pdf</td>\n",
       "      <td>https://paperswithcode.com/paper/legal-judgmen...</td>\n",
       "      <td>[Multi-Task Learning]</td>\n",
       "      <td>[Transfer Learning]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Unsupervised Domain Expansion, Auxiliary Lear...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217275</th>\n",
       "      <td>MovieGraphBenchmark</td>\n",
       "      <td>8054</td>\n",
       "      <td>EAGER: Embedding-Assisted Entity Resolution fo...</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Knowledge Graphs</td>\n",
       "      <td>[Knowledge Base]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Knowledge Graph Completion, Open Knowledge Gr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://arxiv.org/pdf/2101.06126v1.pdf</td>\n",
       "      <td>https://paperswithcode.com/paper/eager-embeddi...</td>\n",
       "      <td>[Entity Resolution, Knowledge Graphs]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Relational Pattern Learning, Knowledge Graph ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Natural Language Processing, Knowledge Base]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217277</th>\n",
       "      <td>OpenEA Benchmark</td>\n",
       "      <td>8055</td>\n",
       "      <td>EAGER: Embedding-Assisted Entity Resolution fo...</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Knowledge Graphs</td>\n",
       "      <td>[Knowledge Base]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Knowledge Graph Completion, Open Knowledge Gr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://arxiv.org/pdf/2101.06126v1.pdf</td>\n",
       "      <td>https://paperswithcode.com/paper/eager-embeddi...</td>\n",
       "      <td>[Entity Resolution, Knowledge Graphs]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Relational Pattern Learning, Knowledge Graph ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Natural Language Processing, Knowledge Base]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92874 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name  pwc_dataset_id  \\\n",
       "0                     MNIST               1   \n",
       "2                    SHAPES            3452   \n",
       "4                     MNIST               1   \n",
       "5                      SVHN             424   \n",
       "6                  CIFAR-10             431   \n",
       "...                     ...             ...   \n",
       "217267                NucMM            8042   \n",
       "217271               AxonEM            8043   \n",
       "217274              MSJudge            8046   \n",
       "217275  MovieGraphBenchmark            8054   \n",
       "217277     OpenEA Benchmark            8055   \n",
       "\n",
       "                                                    title       date  \\\n",
       "0       Modularity Matters: Learning Invariant Relatio... 2018-06-18   \n",
       "2       Modularity Matters: Learning Invariant Relatio... 2018-06-18   \n",
       "4       HitNet: a neural network with capsules embedde... 2018-06-18   \n",
       "5       HitNet: a neural network with capsules embedde... 2018-06-18   \n",
       "6       HitNet: a neural network with capsules embedde... 2018-06-18   \n",
       "...                                                   ...        ...   \n",
       "217267  NucMM Dataset: 3D Neuronal Nuclei Instance Seg... 2021-07-13   \n",
       "217271  AxonEM Dataset: 3D Axon Instance Segmentation ... 2021-07-12   \n",
       "217274  Legal Judgment Prediction with Multi-Stage Cas... 2021-07-12   \n",
       "217275  EAGER: Embedding-Assisted Entity Resolution fo... 2021-01-15   \n",
       "217277  EAGER: Embedding-Assisted Entity Resolution fo... 2021-01-15   \n",
       "\n",
       "        is_problematic  paper_count                   task  \\\n",
       "0                False         4159   Relational Reasoning   \n",
       "2                False           67   Relational Reasoning   \n",
       "4                False         4159      Data Augmentation   \n",
       "5                False         1606      Data Augmentation   \n",
       "6                False         6230      Data Augmentation   \n",
       "...                ...          ...                    ...   \n",
       "217267           False            1  Semantic Segmentation   \n",
       "217271           False            1  Semantic Segmentation   \n",
       "217274           False            1    Multi-Task Learning   \n",
       "217275           False            1       Knowledge Graphs   \n",
       "217277           False            1       Knowledge Graphs   \n",
       "\n",
       "                                               categories  \\\n",
       "0                           [Natural Language Processing]   \n",
       "2                           [Natural Language Processing]   \n",
       "4       [Computer Vision, Natural Language Processing,...   \n",
       "5       [Computer Vision, Natural Language Processing,...   \n",
       "6       [Computer Vision, Natural Language Processing,...   \n",
       "...                                                   ...   \n",
       "217267      [Computer Code, Time Series, Computer Vision]   \n",
       "217271      [Computer Code, Time Series, Computer Vision]   \n",
       "217274                                                 []   \n",
       "217275                                   [Knowledge Base]   \n",
       "217277                                   [Knowledge Base]   \n",
       "\n",
       "                    parents  \\\n",
       "0                        []   \n",
       "2                        []   \n",
       "4                        []   \n",
       "5                        []   \n",
       "6                        []   \n",
       "...                     ...   \n",
       "217267                   []   \n",
       "217271                   []   \n",
       "217274  [Transfer Learning]   \n",
       "217275                   []   \n",
       "217277                   []   \n",
       "\n",
       "                                                 children  \\\n",
       "0                                                      []   \n",
       "2                                                      []   \n",
       "4                 [Image Augmentation, Text Augmentation]   \n",
       "5                 [Image Augmentation, Text Augmentation]   \n",
       "6                 [Image Augmentation, Text Augmentation]   \n",
       "...                                                   ...   \n",
       "217267  [Tumor Segmentation, 3D Semantic Segmentation,...   \n",
       "217271  [Tumor Segmentation, 3D Semantic Segmentation,...   \n",
       "217274                                                 []   \n",
       "217275  [Knowledge Graph Completion, Open Knowledge Gr...   \n",
       "217277  [Knowledge Graph Completion, Open Knowledge Gr...   \n",
       "\n",
       "                                                 siblings  \\\n",
       "0                                                      []   \n",
       "2                                                      []   \n",
       "4                                                      []   \n",
       "5                                                      []   \n",
       "6                                                      []   \n",
       "...                                                   ...   \n",
       "217267                                                 []   \n",
       "217271                                                 []   \n",
       "217274  [Unsupervised Domain Expansion, Auxiliary Lear...   \n",
       "217275                                                 []   \n",
       "217277                                                 []   \n",
       "\n",
       "                                       pdf_url  \\\n",
       "0        http://arxiv.org/pdf/1806.06765v1.pdf   \n",
       "2        http://arxiv.org/pdf/1806.06765v1.pdf   \n",
       "4        http://arxiv.org/pdf/1806.06519v1.pdf   \n",
       "5        http://arxiv.org/pdf/1806.06519v1.pdf   \n",
       "6        http://arxiv.org/pdf/1806.06519v1.pdf   \n",
       "...                                        ...   \n",
       "217267  https://arxiv.org/pdf/2107.05840v1.pdf   \n",
       "217271  https://arxiv.org/pdf/2107.05451v1.pdf   \n",
       "217274  https://arxiv.org/pdf/2107.05192v1.pdf   \n",
       "217275  https://arxiv.org/pdf/2101.06126v1.pdf   \n",
       "217277  https://arxiv.org/pdf/2101.06126v1.pdf   \n",
       "\n",
       "                                                paper_url  \\\n",
       "0       https://paperswithcode.com/paper/modularity-ma...   \n",
       "2       https://paperswithcode.com/paper/modularity-ma...   \n",
       "4       https://paperswithcode.com/paper/hitnet-a-neur...   \n",
       "5       https://paperswithcode.com/paper/hitnet-a-neur...   \n",
       "6       https://paperswithcode.com/paper/hitnet-a-neur...   \n",
       "...                                                   ...   \n",
       "217267  https://paperswithcode.com/paper/nucmm-dataset...   \n",
       "217271  https://paperswithcode.com/paper/axonem-datase...   \n",
       "217274  https://paperswithcode.com/paper/legal-judgmen...   \n",
       "217275  https://paperswithcode.com/paper/eager-embeddi...   \n",
       "217277  https://paperswithcode.com/paper/eager-embeddi...   \n",
       "\n",
       "                                                all_tasks  \\\n",
       "0                [Relational Reasoning, Visual Reasoning]   \n",
       "2                [Relational Reasoning, Visual Reasoning]   \n",
       "4                                     [Data Augmentation]   \n",
       "5                                     [Data Augmentation]   \n",
       "6                                     [Data Augmentation]   \n",
       "...                                                   ...   \n",
       "217267  [Electron Microscopy, Instance Segmentation, R...   \n",
       "217271  [Electron Microscopy, Instance Segmentation, S...   \n",
       "217274                              [Multi-Task Learning]   \n",
       "217275              [Entity Resolution, Knowledge Graphs]   \n",
       "217277              [Entity Resolution, Knowledge Graphs]   \n",
       "\n",
       "                all_parents  \\\n",
       "0                        []   \n",
       "2                        []   \n",
       "4                        []   \n",
       "5                        []   \n",
       "6                        []   \n",
       "...                     ...   \n",
       "217267                   []   \n",
       "217271                   []   \n",
       "217274  [Transfer Learning]   \n",
       "217275                   []   \n",
       "217277                   []   \n",
       "\n",
       "                                             all_children  \\\n",
       "0                          [Visual Commonsense Reasoning]   \n",
       "2                          [Visual Commonsense Reasoning]   \n",
       "4                 [Image Augmentation, Text Augmentation]   \n",
       "5                 [Image Augmentation, Text Augmentation]   \n",
       "6                 [Image Augmentation, Text Augmentation]   \n",
       "...                                                   ...   \n",
       "217267  [Real-Time Semantic Segmentation, Referring Ex...   \n",
       "217271  [Real-Time Semantic Segmentation, Referring Ex...   \n",
       "217274                                                 []   \n",
       "217275  [Relational Pattern Learning, Knowledge Graph ...   \n",
       "217277  [Relational Pattern Learning, Knowledge Graph ...   \n",
       "\n",
       "                                             all_siblings  \\\n",
       "0                                                      []   \n",
       "2                                                      []   \n",
       "4                                                      []   \n",
       "5                                                      []   \n",
       "6                                                      []   \n",
       "...                                                   ...   \n",
       "217267                                                 []   \n",
       "217271                                                 []   \n",
       "217274  [Unsupervised Domain Expansion, Auxiliary Lear...   \n",
       "217275                                                 []   \n",
       "217277                                                 []   \n",
       "\n",
       "                                           all_categories  \n",
       "0       [Reasoning, Natural Language Processing, Compu...  \n",
       "2       [Reasoning, Natural Language Processing, Compu...  \n",
       "4       [Methodology, Natural Language Processing, Com...  \n",
       "5       [Methodology, Natural Language Processing, Com...  \n",
       "6       [Methodology, Natural Language Processing, Com...  \n",
       "...                                                   ...  \n",
       "217267  [Methodology, Time Series, Computer Vision, Co...  \n",
       "217271      [Time Series, Computer Vision, Computer Code]  \n",
       "217274                                                 []  \n",
       "217275      [Natural Language Processing, Knowledge Base]  \n",
       "217277      [Natural Language Processing, Knowledge Base]  \n",
       "\n",
       "[92874 rows x 18 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_citing_papers_pwc.drop_duplicates(['name','title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130ee1b-81c5-4193-8f51-4df61641fe1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5339384542498439"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "49589/92874"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1caaeb7-10e7-4fe9-aa3d-fb5e05d0d12b",
   "metadata": {},
   "source": [
    "# Curating Transfer Datasets\n",
    "\n",
    "Now we're going to combine our four datasets to do dataset transfers. This block merges each citing task-paper with each dataset's task paper. There are three different lists we keep track of:\n",
    "1. The birth of datasets\n",
    "2. Datasets used by papers within the same task\n",
    "3. Datasets used by papers within another tasks\n",
    "\n",
    "I want to clarify some of the challenges here. Each dataset may have multiple \"origin\" tasks, and each using paper might have multiple tasks as well. Moreover, we are primarily interested in higher level-transfers between two parent tasks (e.g. Image Classification to Image Generation, more than \"Few-shot image classification\" to \"genre classification\").\n",
    "\n",
    "If we want to count transfers between tasks we need to make a couple of design decisions:\n",
    "1. A transfer cannot be from one origin task to another.\n",
    "2. An origin does not transfer to a destination's parents, only origins' parents can transfer to destination's parents\n",
    "3. Parents cannot transfer to their own children (this would make sense if two sibiling subtasks are sharing datasets, but we're unable to resolve it).\n",
    "\n",
    "Note this is extremely inefficient and could be embarassingly parallelized using Dask so it could be a lot faster. I would only run this block once and then reload results in the next block..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19f02df6-00c7-4622-b72d-7dfe3e5b4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create infomap inputs\n",
    "\n",
    "# Now create infomap inputs\n",
    "dataset_citing_papers_origins=pd.merge(datasets_pwc,dataset_citing_papers_pwc,on='name')\n",
    "dataset_citing_papers_origins.columns\n",
    "#I realize this is incredibly stupid, I had done it a more elegant way but this works\n",
    "dataset_citing_papers_origins.rename({'title_x':'origin_title',\n",
    "                                     'all_tasks_x':'origin_tasks',\n",
    "                                     'all_parents_x':'origin_parents',\n",
    "                                     'all_siblings_x':'origin_siblings',\n",
    "                                     'all_children_x':'origin_children'},axis=1,inplace=True)\n",
    "dataset_citing_papers_origins.columns=[i.replace('_y','') for i in dataset_citing_papers_origins.columns]\n",
    "dataset_citing_papers_origins=dataset_citing_papers_origins[dataset_citing_papers_origins.title!=dataset_citing_papers_origins.origin_title]\n",
    "\n",
    "task_contains_images={}\n",
    "task_contains_texts={}\n",
    "\n",
    "#external adoptions to siblings and parents\n",
    "sources=[]\n",
    "destinations=[]\n",
    "ds_names=[]\n",
    "paper_titles=[]\n",
    "ds_texts=[]\n",
    "ds_images=[]\n",
    "dest_dates=[]\n",
    "parent_transfer=[]\n",
    "\n",
    "\n",
    "#adoption of homegrown datasets\n",
    "home_tasks=[]\n",
    "home_names=[]\n",
    "home_titles=[]\n",
    "home_dates=[]\n",
    "home_texts=[]\n",
    "home_images=[]\n",
    "home_parent=[]\n",
    "home_introduced=[]\n",
    "\n",
    "#newly created dataset within a task\n",
    "introduced_names=[]\n",
    "introduced_dates=[]\n",
    "introduced_tasks=[]\n",
    "introduced_texts=[]\n",
    "introduced_images=[]\n",
    "introduced_parent=[]\n",
    "introduced_title=[]\n",
    "\n",
    "big_break=False\n",
    "for i,row in dataset_citing_papers_origins.iterrows():\n",
    "    #a valid usage of dataset is restricted to tasks that the dataset is labeled with (or one of their children or siblings).\n",
    "    #The idea here is that many \"usages\" are spurious algorithmic annotations that are mentioned within the paper,\n",
    "    # and restricting ourselves to usages that the dataset and the usage are both labeled with will minimize these spurious usages.\n",
    "    valid_tasks= set(datasets[datasets.name==row['name']]['dataset_tasks'].iloc[0]+\\\n",
    "                     datasets[datasets.name==row['name']]['dataset_tasks_children'].iloc[0]+\\\n",
    "                    datasets[datasets.name==row['name']]['dataset_tasks_siblings'].iloc[0])\n",
    "    for t in row['origin_tasks']:\n",
    "        if t not in valid_tasks: continue\n",
    "        \n",
    "        introduced_names.append(row['name'])\n",
    "        introduced_dates.append(row['introduced_date'])\n",
    "        introduced_tasks.append(t)\n",
    "        introduced_images.append(row['Images'])\n",
    "        introduced_texts.append(row['Texts'])\n",
    "        introduced_title.append(row['origin_title'])\n",
    "        introduced_parent.append(False)\n",
    "        if t not in task_contains_images: task_contains_images[t]=0\n",
    "        if t not in task_contains_texts: task_contains_texts[t]=0\n",
    "        task_contains_texts[t]+=row['Images']; task_contains_images[t]+=row['Texts']\n",
    "\n",
    "        #Your parent gets credit for introducing datasets as well!\n",
    "        for d in row['origin_parents']:\n",
    "            if d not in valid_tasks: continue\n",
    "            introduced_names.append(row['name'])\n",
    "            introduced_dates.append(row['introduced_date'])\n",
    "            introduced_tasks.append(d)\n",
    "            introduced_images.append(row['Images'])\n",
    "            introduced_texts.append(row['Texts'])\n",
    "            introduced_parent.append(True)\n",
    "            introduced_title.append(row['origin_title'])\n",
    "\n",
    "            if d not in task_contains_images: task_contains_images[d]=0\n",
    "            if d not in task_contains_texts: task_contains_texts[d]=0\n",
    "            task_contains_texts[d]+=row['Images']; task_contains_images[d]+=row['Texts']\n",
    "            \n",
    "\n",
    "        #who are you passing it to?\n",
    "        for d in row['all_tasks']:\n",
    "        \n",
    "            if d not in valid_tasks: continue\n",
    "            #if d not in focal_tasks: continue\n",
    "            #Scenario 1: Dest is sources parent\n",
    "            if d in row['origin_parents']: continue\n",
    "            #Scenario 2: Dest is sources child or another origins child:\n",
    "            if d in row['origin_children']: continue\n",
    "            #Scenario 3: Dest is source. (First confirm its not another origin)\n",
    "            if d in row['origin_tasks'] and t!=d: continue\n",
    "            #you've found yourself. Add to home task\n",
    "            if t==d: \n",
    "                home_tasks.append(t)\n",
    "                home_names.append(row['name'])\n",
    "                home_titles.append(row['title'])\n",
    "                home_dates.append(row['date'])\n",
    "                home_images.append(row['Images'])\n",
    "                home_texts.append(row['Texts'])\n",
    "                home_parent.append(False)\n",
    "                home_introduced.append(row['introduced_date'])\n",
    "                if t not in task_contains_images: task_contains_images[t]=0\n",
    "                if t not in task_contains_texts: task_contains_texts[t]=0\n",
    "                task_contains_texts[t]+=row['Images']; task_contains_images[t]+=row['Texts']\n",
    "                #your parents have also homegrown a task\n",
    "                for parent in row['origin_parents']:\n",
    "                    if parent not in valid_tasks: continue\n",
    "                    home_tasks.append(parent)\n",
    "                    home_names.append(row['name'])\n",
    "                    home_titles.append(row['title'])\n",
    "                    home_dates.append(row['date'])\n",
    "                    home_images.append(row['Images'])\n",
    "                    home_texts.append(row['Texts'])\n",
    "                    home_parent.append(True)\n",
    "                    home_introduced.append(row['introduced_date'])\n",
    "                    if parent not in task_contains_images: task_contains_images[parent]=0\n",
    "                    if parent not in task_contains_texts: task_contains_texts[parent]=0\n",
    "                    task_contains_texts[parent]+=row['Images']; task_contains_images[parent]+=row['Texts']\n",
    "            #you haven't found yourself and this is a transfer\n",
    "            else:\n",
    "                #A. pass directly to this tasks\n",
    "                if t not in valid_tasks or d not in valid_tasks: continue\n",
    "                sources.append(t)\n",
    "                destinations.append(d)\n",
    "                ds_names.append(row['name'])\n",
    "                paper_titles.append(row['title'])\n",
    "                dest_dates.append(row['date'])\n",
    "                ds_texts.append(row['Texts'])\n",
    "                ds_images.append(row['Images'])\n",
    "                parent_transfer.append(False)\n",
    "                if t not in task_contains_images: task_contains_images[t]=0\n",
    "                if t not in task_contains_texts: task_contains_texts[t]=0\n",
    "                task_contains_texts[t]+=row['Images']; task_contains_images[t]+=row['Texts']\n",
    "                if d not in task_contains_images: task_contains_images[d]=0\n",
    "                if d not in task_contains_texts: task_contains_texts[d]=0\n",
    "                task_contains_texts[d]+=row['Images']; task_contains_images[d]+=row['Texts'] \n",
    "                \n",
    "                #Now pass between parent tasks\n",
    "                for pt in row['origin_parents']:\n",
    "                    if t not in valid_tasks or pt not in valid_tasks: continue\n",
    "                    if t==pt: continue #cant transfer to yourself (this only occurs with self-loops)                        \n",
    "                    for pdest in row['all_parents']:\n",
    "                        if t not in valid_tasks or pdest not in valid_tasks: continue\n",
    "                        #cant transfer to yourself to your own parent or children\n",
    "                        #this is a simplification because in reality, you can transfer to your children but we lack the resolution to resolve that\n",
    "                        if t==pdest or pdest in row['origin_parents'] or pdest in row['origin_children']: continue\n",
    "                        sources.append(pt)\n",
    "                        destinations.append(pdest)\n",
    "                        ds_names.append(row['name'])\n",
    "                        paper_titles.append(row['title'])\n",
    "                        dest_dates.append(row['date'])\n",
    "                        ds_texts.append(row['Texts'])\n",
    "                        ds_images.append(row['Images'])\n",
    "                        parent_transfer.append(True)\n",
    "                        if pt not in task_contains_images: task_contains_images[pt]=0\n",
    "                        if pt not in task_contains_texts: task_contains_texts[pt]=0\n",
    "                        task_contains_texts[pt]+=row['Images']; task_contains_images[pt]+=row['Texts']\n",
    "                        if pdest not in task_contains_images: task_contains_images[pdest]=0\n",
    "                        if pdest not in task_contains_texts: task_contains_texts[pdest]=0\n",
    "                        task_contains_texts[pdest]+=row['Images']; task_contains_images[pdest]+=row['Texts']\n",
    "\n",
    "source_dest_edgelist=pd.DataFrame({'source_task':sources,'dest_task':destinations,'name':ds_names,'title':paper_titles,'date':dest_dates,'Images':ds_images,'Texts':ds_texts,'Parent_Transfer':parent_transfer}).drop_duplicates()\n",
    "homegrown_edgelist=pd.DataFrame({'task':home_tasks,'name':home_names,'title':home_titles,'date':home_dates,'Images':home_images,'Texts':home_texts,'Parent':home_parent,'introduced_date':home_introduced}).drop_duplicates()\n",
    "birth_edgelist=pd.DataFrame({'task':introduced_tasks,'name':introduced_names,'title':introduced_title,'date':introduced_dates,'Images':introduced_images,'Texts':introduced_texts,'Parent':introduced_parent}).drop_duplicates()\n",
    "task_contains_images=pd.Series(task_contains_images)\n",
    "task_contains_images[task_contains_images>0]=1\n",
    "task_contains_texts=pd.Series(task_contains_texts)\n",
    "task_contains_texts[task_contains_texts>0]=1\n",
    "task_contains_images=task_contains_images.reset_index()\n",
    "task_contains_images.columns=['task','Images']\n",
    "task_contains_texts=task_contains_texts.reset_index()\n",
    "task_contains_texts.columns=['task','Texts']\n",
    "source_dest_edgelist.to_csv('./PWC_Data/Derivative_Datasets/source_dest_edgelist.csv',quoting=1,index=False)\n",
    "homegrown_edgelist.to_csv('./PWC_Data/Derivative_Datasets/homegrown_edgelist',quoting=1,index=False)\n",
    "birth_edgelist.to_csv('./PWC_Data/Derivative_Datasets/birth_edgelist.csv',quoting=1,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6e59a99a-2ba5-4336-87e1-745d880859ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>homepage</th>\n",
       "      <th>description</th>\n",
       "      <th>introduced_date</th>\n",
       "      <th>warning</th>\n",
       "      <th>modalities</th>\n",
       "      <th>languages</th>\n",
       "      <th>variants</th>\n",
       "      <th>...</th>\n",
       "      <th>parents</th>\n",
       "      <th>children</th>\n",
       "      <th>siblings</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>date</th>\n",
       "      <th>all_tasks</th>\n",
       "      <th>all_parents</th>\n",
       "      <th>all_children</th>\n",
       "      <th>all_siblings</th>\n",
       "      <th>all_categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [url, name, full_name, homepage, description, introduced_date, warning, modalities, languages, variants, num_papers, data_loaders, title, Texts, Images, dataset_tasks, dataset_tasks_parents, dataset_tasks_categories, dataset_tasks_children, dataset_tasks_siblings, task, categories, parents, children, siblings, pdf_url, date, all_tasks, all_parents, all_children, all_siblings, all_categories]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 32 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f61d43e9-4c67-4cd6-b4a7-4f9b607c21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dest_edgelist=pd.read_csv('./PWC_Data/Derivative_Datasets/source_dest_edgelist.csv')\n",
    "try: source_dest_edgelist=source_dest_edgelist.drop('Unnamed: 0',axis=1) #incase you added an index...\n",
    "except: pass\n",
    "source_dest_edgelist['date']=pd.to_datetime(source_dest_edgelist.date)\n",
    "\n",
    "homegrown_edgelist=pd.read_csv('./PWC_Data/Derivative_Datasets/homegrown_edgelist')\n",
    "try: homegrown_edgelist=homegrown_edgelist.drop('Unnamed: 0',axis=1) #incase you added an index...\n",
    "except: pass\n",
    "homegrown_edgelist['date']=pd.to_datetime(homegrown_edgelist.date)\n",
    "\n",
    "birth_edgelist=pd.read_csv('./PWC_Data/Derivative_Datasets/birth_edgelist.csv')\n",
    "try: birth_edgelist=birth_edgelist.drop('Unnamed: 0',axis=1) #incase you added an index...\n",
    "except: pass\n",
    "birth_edgelist['date']=pd.to_datetime(birth_edgelist.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34908600-70de-4143-964a-88af4edcb159",
   "metadata": {},
   "source": [
    "# RQ2: Creating ratio datasets\n",
    "We create the dataset for RQ2 first because we use the same tasks\n",
    "Below are three largely similar blocks that create ratio datasets:\n",
    "    \n",
    "1. Calculates ratios for transfers between parent tasks, aggregated across all years (Figure 2)\n",
    "2. Calculates ratios for transfers between parent tasks, disaggregated by year (Figure 1)\n",
    "3. Calculates ratios for transfers between all tasks, disaggregated by year (not shown)\n",
    "\n",
    "We do not use 3 because it double counts transfers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a359e-7545-4978-a43d-ea704f1cbf68",
   "metadata": {},
   "source": [
    "## Counting events\n",
    "When considering dataset births, usages, and transfers by task, we chose to limit ourselves to parent tasks to avoid biasing our dataset with small tasks on the leaves of the ontology.\n",
    "Our rationale is that some of these leaf tasks were either named somewhat idiosyncratically or VERY particular. \n",
    "If you want to see what they are you can run this: `[print(i) for i in parent_child_dict if len(parent_child_dict[i])==0]`\n",
    "\n",
    "At the same time we didn't want to exclude tasks that had no parents themselves, so we just use any transfer in our dataset where both the source and destination tasks are parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "146b9e78-5192-4fe9-ab69-179c2e56f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_tasks=[i for i in parent_child_dict if len(parent_child_dict[i])!=0]\n",
    "\n",
    "try: source_dest_edgelist= source_dest_edgelist.drop('Parent_Transfer',axis=1).drop_duplicates()\n",
    "except: pass\n",
    "source_dest_edgelist_parents= source_dest_edgelist[(source_dest_edgelist.source_task.isin(parent_tasks)) &\\\n",
    "                                                  (source_dest_edgelist.dest_task.isin(parent_tasks))]\n",
    "try: homegrown_edgelist= homegrown_edgelist.drop('Parent',axis=1).drop_duplicates()\n",
    "except: pass\n",
    "homegrown_edgelist_parents= homegrown_edgelist[homegrown_edgelist.task.isin(parent_tasks)]\n",
    "\n",
    "try: birth_edgelist= birth_edgelist.drop('Parent',axis=1).drop_duplicates()\n",
    "except: pass\n",
    "birth_edgelist_parents= birth_edgelist[birth_edgelist.task.isin(parent_tasks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35746e-fd46-4001-a15c-c9fee09e523a",
   "metadata": {},
   "source": [
    "In this block we count the number of different types of events for each tasks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a603a721-32db-4350-b817-14199c327ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets born and then used at least once:  1056\n",
      "Total number of unique usages within introducing paper task:  22115\n",
      "Total number of unique usages from outside paper-introducing task:  14646\n",
      "Total number of parent tasks involved:  269\n",
      "Total number of papers involved:  21814\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of datasets born and then used at least once: \",birth_edgelist_parents['name'].drop_duplicates().shape[0])\n",
    "print(\"Total number of unique usages within introducing paper task: \", homegrown_edgelist_parents[['name','title']].drop_duplicates().shape[0])\n",
    "print(\"Total number of unique usages from outside paper-introducing task: \", source_dest_edgelist_parents[['name','title']].drop_duplicates().shape[0])\n",
    "print(\"Total number of parent tasks involved: \",pd.concat([birth_edgelist_parents['task'],homegrown_edgelist_parents['task'],\n",
    "           source_dest_edgelist_parents['source_task'],source_dest_edgelist_parents['dest_task']]).drop_duplicates().shape[0])\n",
    "print(\"Total number of papers involved: \",pd.concat([birth_edgelist_parents['title'],homegrown_edgelist_parents['title'],\n",
    "           source_dest_edgelist_parents['title'],source_dest_edgelist_parents['title']]).drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5189814e-9ff1-4d58-b54f-d2957696c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We dropped these methodologies because we thought they were innapropriate for this category\n",
    "Methodologies_to_Drop=[\n",
    "'Word Embeddings',\n",
    "'Anomaly Detection',\n",
    "'Multivariate Time Series Forecasting',\n",
    "'EEG',\n",
    "'Chatbot',\n",
    "'Computed Tomography (CT)',\n",
    "'Electrocardiography (ECG)',\n",
    "'Electrocardiography (ECG)',\n",
    "'Multi-Label Text Classification'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dc9b6992-dc7e-4b92-88ef-e511661bc493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median parent task size:  34.0\n",
      "Number of tasks for final analysis (Figure 2):  133\n"
     ]
    }
   ],
   "source": [
    "#COUNT THE NUMBER OF PAPERS USED WITHIN THE TASK\n",
    "num_papers_adopting=source_dest_edgelist_parents.groupby(['dest_task']).size() #counts the number of adopting papers within tasks\n",
    "num_papers_adopting.index.names=['task']\n",
    "num_papers_growing=homegrown_edgelist_parents.groupby(['task']).size() #counts the number of papers that use a \"homegrown\" dataset within tasks\n",
    "num_dataset_births=birth_edgelist_parents.groupby(['task']).size() #number of datasets created within a task that are used at least once\n",
    "\n",
    "#this is counting number of unique datasets that are imported in each task\n",
    "temp=source_dest_edgelist_parents.groupby(['dest_task','name']).size().reset_index().drop(0,axis=1)\n",
    "num_dataset_imports=temp.groupby(['dest_task']).size() #note that we are just counting datasets here with size\n",
    "\n",
    "num_dataset_imports.columns=['imported_datasets']\n",
    "num_dataset_imports.index.names=['task']\n",
    "num_dataset_births.name='num_dataset_births'\n",
    "num_papers_adopting.name='num_papers_adopting'\n",
    "num_dataset_imports.name='num_dataset_imports'\n",
    "num_papers_growing.name='num_papers_growing'\n",
    "\n",
    "#in this section we merge four different types of counts:\n",
    "#number of dataset births within a task and number of datasets adopted in each task\n",
    "full_data=pd.merge(num_dataset_births.reset_index(),num_papers_adopting.reset_index(),how='outer')\n",
    "full_data=pd.merge(full_data,num_dataset_imports.reset_index(),how='outer') #number of unique datasets imported\n",
    "full_data=pd.merge(full_data,num_papers_growing.reset_index(),how='outer') #number of usages of homegrown datasets\n",
    "full_data=full_data.fillna(0)\n",
    "\n",
    "#this size is the total number of usages (not papers in each task)\n",
    "full_data['size']=full_data.num_papers_adopting+full_data.num_papers_growing+full_data.num_dataset_births\n",
    "task_age_df=task_age.reset_index()\n",
    "task_age_df.columns=['task','task_age']\n",
    "full_data=pd.merge(full_data,task_age_df,on='task',how='left')\n",
    "\n",
    "#We dont use these ratios in the paper because they are more numerically unstable\n",
    "full_data['adoption_ratio']=full_data.num_papers_adopting.divide(full_data.num_papers_growing)\n",
    "full_data['creation_ratio']=full_data.num_dataset_births.divide(full_data.num_dataset_imports)\n",
    "\n",
    "#same as above but adding the numerator to the denominator to add stability\n",
    "full_data['adoption_pct']=full_data.num_papers_adopting.divide(full_data.num_papers_adopting+full_data.num_papers_growing)\n",
    "full_data['creation_pct']=full_data.num_dataset_births.divide(full_data.num_dataset_births+full_data.num_dataset_imports)\n",
    "\n",
    "\n",
    "def in_category(x,cat):\n",
    "    if x in task_category_dict and cat in task_category_dict[x]:\n",
    "        return 1\n",
    "    if x in parent_child_dict and any([cat in task_category_dict[p] for p in child_parent_dict[x]]):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "#these categories are provided by PWC\n",
    "full_data['CV']=full_data['task'].apply(lambda x: in_category(x,'Computer Vision'))\n",
    "full_data['NLP']=full_data['task'].apply(lambda x: in_category(x,'Natural Language Processing'))\n",
    "full_data['Methodology']=full_data['task'].apply(lambda x: in_category(x,'Methodology'))\n",
    "full_data['Methodology']=full_data['Methodology'].apply(lambda x: 0 if x in Methodologies_to_Drop else x)\n",
    "\n",
    "'''\n",
    "In this block we restrict our analysis to the 50% largest tasks.\n",
    "This is because our ratios become very unstable when we disaggregate by year for smaller tasks.\n",
    "Moreover, I'm not sure we're interested in tasks with <34 usages anyways.\n",
    "Note that we use the same 134 tasks in RQ1 as well.\n",
    "'''\n",
    "median_parent_task_size=full_data['size'].median()\n",
    "full_data=full_data[full_data['size']>median_parent_task_size]\n",
    "print(\"Median parent task size: \",median_parent_task_size)\n",
    "print(\"Number of tasks for final analysis (Figure 2): \",full_data['task'].unique().shape[0])\n",
    "median_parent_tasks=full_data.task\n",
    "median_parent_tasks.to_csv('./PWC_Data/Derivative_Datasets/median_parent_tasks.txt')\n",
    "full_data.to_csv(\"./PWC_Data/Derivative_Datasets/FullDatasetforR.ParentsOnly.AllYears.NotMedians.txt\",sep='\\t',quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd637b-7ae5-4b00-a8ec-7d8d8844030f",
   "metadata": {},
   "source": [
    "This block does the same thing as above but disaggregates by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "403ba543-ff25-44f1-889f-555e699ac91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNT THE NUMBER OF PAPERS USED WITHIN THE TASK BY YEAR\n",
    "num_papers_adopting=source_dest_edgelist_parents.groupby(['dest_task',source_dest_edgelist_parents.date.dt.year]).size()\n",
    "num_papers_adopting.index.names=['task','date']\n",
    "num_papers_growing=homegrown_edgelist_parents.groupby(['task',homegrown_edgelist_parents.date.dt.year]).size()\n",
    "num_dataset_births=birth_edgelist_parents.groupby(['task', birth_edgelist_parents.date.dt.year]).size()\n",
    "\n",
    "#this is counting number of unique datasets that are imported in each task\n",
    "temp=source_dest_edgelist_parents.groupby(['dest_task','name',source_dest_edgelist_parents.date.dt.year]).size().reset_index().drop(0,axis=1)\n",
    "num_dataset_imports=temp.groupby(['dest_task',source_dest_edgelist_parents.date.dt.year]).size()\n",
    "\n",
    "num_dataset_imports.columns=['imported_datasets']\n",
    "num_dataset_imports.index.names=['task','date']\n",
    "num_dataset_births.name='num_dataset_births'\n",
    "num_papers_adopting.name='num_papers_adopting'\n",
    "num_dataset_imports.name='num_dataset_imports'\n",
    "num_papers_growing.name='num_papers_growing'\n",
    "\n",
    "#merge these counts\n",
    "annual_data=pd.merge(num_dataset_births.reset_index(),num_papers_adopting.reset_index(),how='outer')\n",
    "annual_data=pd.merge(annual_data,num_dataset_imports.reset_index(),how='outer')\n",
    "annual_data=pd.merge(annual_data,num_papers_growing.reset_index(),how='outer')\n",
    "annual_data=annual_data.fillna(0)\n",
    "\n",
    "annual_data['size']=annual_data.num_papers_adopting+annual_data.num_papers_growing+annual_data.num_dataset_births\n",
    "task_age_df=task_age.reset_index()\n",
    "task_age_df.columns=['task','task_age']\n",
    "annual_data=pd.merge(annual_data,task_age_df,on='task',how='left')\n",
    "annual_data['adoption_ratio']=annual_data.num_papers_adopting.divide(annual_data.num_papers_growing)\n",
    "annual_data['creation_ratio']=annual_data.num_dataset_births.divide(annual_data.num_dataset_imports)\n",
    "#annual_data['conversion_ratio']=annual_data.num_dataset_homegrown.divide(annual_data.num_papers_growing)\n",
    "annual_data['adoption_pct']=annual_data.num_papers_adopting.divide(annual_data.num_papers_adopting+annual_data.num_papers_growing)\n",
    "annual_data['creation_pct']=annual_data.num_dataset_births.divide(annual_data.num_dataset_births+annual_data.num_dataset_imports)\n",
    "\n",
    "pwc_papers['year']=pd.to_datetime(pwc_papers['date']).dt.year\n",
    "annual_size=pwc_papers.groupby('year').size().reset_index()\n",
    "annual_data=pd.merge(annual_data,annual_size,left_on='date',right_on='year',how='left')\n",
    "annual_data=annual_data.drop('year',axis=1).rename({0:'pwc_size'},axis=1)\n",
    "annual_data.rename({'date':'year'},axis=1,inplace=True)\n",
    "\n",
    "def in_category(x,cat):\n",
    "    if x in task_category_dict and cat in task_category_dict[x]:\n",
    "        return 1\n",
    "    if x in parent_child_dict and any([cat in task_category_dict[p] for p in child_parent_dict[x]]):\n",
    "        return 1\n",
    "    return 0\n",
    "annual_data['CV']=annual_data['task'].apply(lambda x: in_category(x,'Computer Vision'))\n",
    "annual_data['NLP']=annual_data['task'].apply(lambda x: in_category(x,'Natural Language Processing'))\n",
    "annual_data['Methodology']=annual_data['task'].apply(lambda x: in_category(x,'Methodology'))\n",
    "annual_data['Methodology']=annual_data['Methodology'].apply(lambda x: 0 if x in Methodologies_to_Drop else x)\n",
    "#THESE ARE ERRONEOUS NEED TO CHECK EACH ERROR\n",
    "annual_data=annual_data[annual_data.year>=annual_data.task_age]\n",
    "annual_data=annual_data[annual_data.task.isin(median_parent_tasks)]\n",
    "annual_data.to_csv(\"./PWC_Data/Derivative_Datasets/FullDatasetforR.ParentsOnly.txt\",sep='\\t',quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1b555-49ae-4e2c-9f91-ff3c7d160462",
   "metadata": {},
   "source": [
    "This block imposes no restrictions on tasks but if we're not using only parent transfers, that means there can be double counts at diferent levels of organization...\n",
    "It's not used in the paper but the results are similar (CHECK TO MAKE SURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de8e7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_papers_adopting=source_dest_edgelist.groupby(['dest_task',source_dest_edgelist.date.dt.year]).size()\n",
    "num_papers_adopting.index.names=['task','date']\n",
    "num_papers_growing=homegrown_edgelist.groupby(['task',homegrown_edgelist.date.dt.year]).size()\n",
    "num_dataset_births=birth_edgelist.groupby(['task', birth_edgelist.date.dt.year]).size()\n",
    "num_homegrown_datasets=num_dataset_births.shift(1).cumsum()\n",
    "temp=source_dest_edgelist.groupby(['dest_task','name',source_dest_edgelist.date.dt.year]).size().reset_index().drop(0,axis=1)\n",
    "num_dataset_imports=temp.groupby(['dest_task',source_dest_edgelist.date.dt.year]).size()\n",
    "\n",
    "num_dataset_imports.columns=['imported_datasets']\n",
    "num_dataset_imports.index.names=['task','date']\n",
    "num_dataset_births.name='num_dataset_births'\n",
    "num_papers_adopting.name='num_papers_adopting'\n",
    "num_dataset_imports.name='num_dataset_imports'\n",
    "num_papers_growing.name='num_papers_growing'\n",
    "num_homegrown_datasets.name='num_dataset_homegrown'\n",
    "\n",
    "annual_data=pd.merge(num_dataset_births.reset_index(),num_papers_adopting.reset_index(),how='outer')\n",
    "annual_data=pd.merge(annual_data,num_homegrown_datasets.reset_index(),how='outer')\n",
    "annual_data=pd.merge(annual_data,num_dataset_imports.reset_index(),how='outer')\n",
    "annual_data=pd.merge(annual_data,num_papers_growing.reset_index(),how='outer')\n",
    "annual_data=annual_data.fillna(0)\n",
    "\n",
    "annual_data['size']=annual_data.num_papers_adopting+annual_data.num_papers_growing+annual_data.num_dataset_births\n",
    "\n",
    "task_age_df=task_age.reset_index()\n",
    "task_age_df.columns=['task','task_age']\n",
    "annual_data=pd.merge(annual_data,task_age_df,on='task',how='left')\n",
    "annual_data['adoption_ratio']=annual_data.num_papers_adopting.divide(annual_data.num_papers_growing)\n",
    "annual_data['creation_ratio']=annual_data.num_dataset_births.divide(annual_data.num_dataset_imports)\n",
    "\n",
    "annual_data['adoption_pct']=annual_data.num_papers_adopting.divide(annual_data.num_papers_adopting+annual_data.num_papers_growing)\n",
    "annual_data['creation_pct']=annual_data.num_dataset_births.divide(annual_data.num_dataset_births+annual_data.num_dataset_imports)\n",
    "\n",
    "\n",
    "pwc_papers['year']=pd.to_datetime(pwc_papers['date']).dt.year\n",
    "annual_size=pwc_papers.groupby('year').size().reset_index()\n",
    "annual_data=pd.merge(annual_data,annual_size,left_on='date',right_on='year',how='left')\n",
    "annual_data=annual_data.drop('year',axis=1).rename({0:'pwc_size'},axis=1)\n",
    "annual_data.rename({'date':'year'},axis=1,inplace=True)\n",
    "\n",
    "def in_category(x,cat):\n",
    "    if x in task_category_dict and cat in task_category_dict[x]:\n",
    "        return 1\n",
    "    if x in parent_child_dict and any([cat in task_category_dict[p] for p in child_parent_dict[x]]):\n",
    "        return 1\n",
    "    return 0\n",
    "annual_data['CV']=annual_data['task'].apply(lambda x: in_category(x,'Computer Vision'))\n",
    "annual_data['NLP']=annual_data['task'].apply(lambda x: in_category(x,'Natural Language Processing'))\n",
    "annual_data['Methodology']=annual_data['task'].apply(lambda x: in_category(x,'Methodology'))\n",
    "annual_data['Methodology']=annual_data['Methodology'].apply(lambda x: 0 if x in Methodologies_to_Drop else x)\n",
    "annual_data=annual_data[annual_data.year>=annual_data.task_age]\n",
    "annual_data.to_csv(\"./PWC_Data/Derivative_Datasets/FullDatasetforR.txt\",sep='\\t',quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0e573-e99a-41a8-b931-e86cad3dd919",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create Gini Data for RQ 1\n",
    "This block goes through the datasets DF and filters out usages (invalid tasks) that are not related to a task the dataset was annotated for. It also adds parent tasks as usages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4887636d-0e97-4f0e-9a1c-5ac455d19897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks:  1397\n",
      "Number of datasets:  2931\n",
      "Number of papers:  32947\n"
     ]
    }
   ],
   "source": [
    "dataset_name=[]\n",
    "paper_title=[]\n",
    "paper_date=[]\n",
    "paper_tasks=[]\n",
    "paper_CV=[]\n",
    "paper_NLP=[]\n",
    "paper_Methods=[]\n",
    "paper_parent=[]\n",
    "def in_category(x,cat):\n",
    "    if x in task_category_dict and cat in task_category_dict[x]:\n",
    "        return 1\n",
    "    if x in parent_child_dict and any([cat in task_category_dict[p] for p in child_parent_dict[x]]):\n",
    "        return 1\n",
    "    return 0\n",
    "for i, row in dataset_citing_papers_pwc.drop_duplicates(['name','title']).iterrows():\n",
    "    #again we're skipping this dataset because there is some wonky labeling in PWC\n",
    "    if row['name']=='PRID2011':continue\n",
    "    #restrict ourselves to valid tasks\n",
    "    valid_tasks= set(datasets_pwc_total[datasets_pwc_total.name==row['name']]['dataset_tasks'].iloc[0]+\\\n",
    "                     datasets_pwc_total[datasets_pwc_total.name==row['name']]['dataset_tasks_children'].iloc[0]+\\\n",
    "                     datasets_pwc_total[datasets_pwc_total.name==row['name']]['dataset_tasks_siblings'].iloc[0]\n",
    "                    )\n",
    "    for t in row['all_tasks']:\n",
    "        if t not in valid_tasks: continue \n",
    "        dataset_name.append(row['name'])\n",
    "        paper_title.append(row['title'])\n",
    "        paper_date.append(row['date'].year)\n",
    "        paper_tasks.append(t)\n",
    "        paper_parent.append(False)\n",
    "    for t in row['all_parents']:\n",
    "        if t not in valid_tasks: continue\n",
    "        dataset_name.append(row['name'])\n",
    "        paper_title.append(row['title'])\n",
    "        paper_date.append(row['date'].year)\n",
    "        paper_tasks.append(t)\n",
    "        paper_parent.append(True)\n",
    "entropy_dataset=pd.DataFrame({'task':paper_tasks,'name':dataset_name,'title':paper_title,'date':paper_date}).drop_duplicates()\n",
    "print(\"Number of tasks: \",entropy_dataset.task.drop_duplicates().shape[0])\n",
    "print(\"Number of datasets: \",entropy_dataset.name.drop_duplicates().shape[0])\n",
    "print(\"Number of papers: \",entropy_dataset.title.drop_duplicates().shape[0])\n",
    "entropy_dataset.to_csv('../Dataset_Curation/EntropyDataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "19bcc1b7-bea8-4110-b391-7baffa6d1ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>Dataset_Name</th>\n",
       "      <th>count</th>\n",
       "      <th>total</th>\n",
       "      <th>percent</th>\n",
       "      <th>gini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>ImageNet</td>\n",
       "      <td>1721</td>\n",
       "      <td>23440</td>\n",
       "      <td>0.0734215</td>\n",
       "      <td>0.800129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>COCO</td>\n",
       "      <td>1173</td>\n",
       "      <td>23440</td>\n",
       "      <td>0.0500427</td>\n",
       "      <td>0.800129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>MNIST</td>\n",
       "      <td>1036</td>\n",
       "      <td>23440</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.800129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>KITTI</td>\n",
       "      <td>460</td>\n",
       "      <td>23440</td>\n",
       "      <td>0.0196246</td>\n",
       "      <td>0.800129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>448</td>\n",
       "      <td>23440</td>\n",
       "      <td>0.0191126</td>\n",
       "      <td>0.800129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>2011</td>\n",
       "      <td>AIDA CoNLL-YAGO</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0357143</td>\n",
       "      <td>0.385714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4757</th>\n",
       "      <td>2011</td>\n",
       "      <td>AwA</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0357143</td>\n",
       "      <td>0.385714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758</th>\n",
       "      <td>2011</td>\n",
       "      <td>Caltech-256</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0357143</td>\n",
       "      <td>0.385714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4759</th>\n",
       "      <td>2011</td>\n",
       "      <td>Penn Treebank</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0357143</td>\n",
       "      <td>0.385714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4760</th>\n",
       "      <td>2011</td>\n",
       "      <td>SBU Captions Dataset</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0357143</td>\n",
       "      <td>0.385714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4761 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year          Dataset_Name  count  total    percent      gini\n",
       "0     2020              ImageNet   1721  23440  0.0734215  0.800129\n",
       "1     2020                  COCO   1173  23440  0.0500427  0.800129\n",
       "2     2020                 MNIST   1036  23440   0.044198  0.800129\n",
       "3     2020                 KITTI    460  23440  0.0196246  0.800129\n",
       "4     2020            Cityscapes    448  23440  0.0191126  0.800129\n",
       "...    ...                   ...    ...    ...        ...       ...\n",
       "4756  2011       AIDA CoNLL-YAGO      1     28  0.0357143  0.385714\n",
       "4757  2011                   AwA      1     28  0.0357143  0.385714\n",
       "4758  2011           Caltech-256      1     28  0.0357143  0.385714\n",
       "4759  2011         Penn Treebank      1     28  0.0357143  0.385714\n",
       "4760  2011  SBU Captions Dataset      1     28  0.0357143  0.385714\n",
       "\n",
       "[4761 rows x 6 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf4195-6139-4458-af9d-53caa323dcc9",
   "metadata": {},
   "source": [
    "Optional block: Sanity check for how many extra usages we are adding back via manually labeling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f92f63cc-de98-43f1-9047-a260f2cd27bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of usages recovered through manual annotation: 21397\n",
      "Number of usages still tossed: 10014\n"
     ]
    }
   ],
   "source": [
    "sheet_id = '1Y3DDI6ySi9A6l3ZMET29EWSxBr8Uw-kvKn8RF2zYpzQ'\n",
    "sheet_name = 'untasked_datasets'\n",
    "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "manual_task_labels=pd.read_csv(url)\n",
    "manual_tasks_not_labeled=manual_task_labels[manual_task_labels.Justification.isnull()]\n",
    "manual_task_labels=manual_task_labels[~manual_task_labels.Justification.isnull()]\n",
    "#manual_tasks_not_labeled=manual_task_labels.iloc[75:,]\n",
    "#manual_task_labels=manual_task_labels.iloc[:75,]\n",
    "entropy_dataset=pd.read_csv('../Dataset_Curation/EntropyDataset.txt')\n",
    "entropy_top_tasks=entropy_dataset[entropy_dataset.task.isin(median_parent_tasks)]\n",
    "usages_top_tasks=dataset_citing_papers_pwc[dataset_citing_papers_pwc.task.isin(median_parent_tasks)]\n",
    "print(\"Number of usages recovered through manual annotation:\", entropy_top_tasks[entropy_top_tasks.name.isin(manual_task_labels.name)].shape[0])\n",
    "print(\"Number of usages still tossed:\",usages_top_tasks[usages_top_tasks.name.isin(manual_tasks_not_labeled.name)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c6cb1-603b-4ac1-8410-f167ab42339b",
   "metadata": {},
   "source": [
    "These are the metrics used in the analyses. Note we do not report the Pielou evenness in the paper, which is information-theoretic metric, but performs similarly to Gini.\n",
    "\n",
    "I also experimented with the Simpson index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1fda5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from math import log\n",
    "from skbio.diversity import alpha\n",
    "#right now I am just saying having 1 or 0 datasets is meaningless\n",
    "def gini(x):\n",
    "    if len(x)<2:return None\n",
    "    x=np.array(x)\n",
    "    \"\"\"Compute Gini coefficient of array of values\"\"\"\n",
    "    diffsum = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        diffsum += np.sum(np.abs(xi - x[i:]))\n",
    "    return diffsum / (len(x)**2 * np.mean(x))\n",
    "\n",
    "def corrected_gini(x):\n",
    "    if len(x)<2:return None\n",
    "    x=np.array(x)\n",
    "    \"\"\"Compute Gini coefficient of array of values\"\"\"\n",
    "    diffsum = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        diffsum += np.sum(np.abs(xi - x[i:]))\n",
    "    gini = diffsum / (len(x)**2 * np.mean(x))\n",
    "    return len(x)*gini/(len(x)-1) \n",
    "\n",
    "def pielou(x):\n",
    "    if len(x)<2:return None\n",
    "    x=np.array(x)\n",
    "    return entropy(x,base=2)/log(len(x),2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac3cea-03f7-416b-a4de-875f907838d6",
   "metadata": {},
   "source": [
    "Main dataset used in the paper (RQ1): Inequality metrics for parent tasks greater than the median size..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c3dc1d43-11cf-4aef-9e35-5e1dbe2a24ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks (parent-tasks only):  133\n",
      "Number of datasets (parent-tasks only):  2063\n",
      "Number of usages (parent-tasks only):  49008\n",
      "Number of papers (parent-tasks only):  26691\n"
     ]
    }
   ],
   "source": [
    "temp=entropy_dataset[entropy_dataset.task.isin(median_parent_tasks)]\n",
    "print(\"Number of tasks (parent-tasks only): \",temp.task.drop_duplicates().shape[0])\n",
    "print(\"Number of datasets (parent-tasks only): \",temp.name.drop_duplicates().shape[0])\n",
    "print(\"Number of usages (parent-tasks only): \",temp.drop_duplicates(['name','title']).shape[0])\n",
    "print(\"Number of papers (parent-tasks only): \",temp.title.drop_duplicates().shape[0])\n",
    "\n",
    "task_ds_ycounts=entropy_dataset.groupby(['task','name',entropy_dataset.date]).size().reset_index()\n",
    "#this is the important line\n",
    "task_ds_ycounts= task_ds_ycounts[task_ds_ycounts.task.isin(median_parent_tasks)]\n",
    "#task_ds_ycounts= task_ds_ycounts[task_ds_ycounts.task.isin(parent_tasks)]\n",
    "\n",
    "task_ds_ycounts.columns=['task', 'name','year','count']\n",
    "ginis=task_ds_ycounts.groupby(['task','year'])['count'].agg(corrected_gini).reset_index()\n",
    "ginis.columns=['task','year','gini']\n",
    "pielous=task_ds_ycounts.groupby(['task','year'])['count'].agg(pielou).reset_index()\n",
    "pielous.columns=['task','year','pielou']\n",
    "simpson=task_ds_ycounts.groupby(['task','year'])['count'].agg(alpha.simpson_e).reset_index()\n",
    "simpson.columns=['task','year','simpson']\n",
    "size=task_ds_ycounts.groupby(['task','year'])['count'].sum().reset_index()\n",
    "size.columns=['task','year','task_size']\n",
    "inequity_years_df=pd.merge(ginis,pielous,on=['task','year'])\n",
    "inequity_years_df=pd.merge(inequity_years_df,simpson,on=['task','year'],how='left')\n",
    "inequity_years_df=pd.merge(inequity_years_df,size,on=['task','year'],how='left')\n",
    "task_age_df=task_age.reset_index()\n",
    "task_age_df.columns=['task','task_age']\n",
    "inequity_years_df=pd.merge(inequity_years_df,task_age_df,on='task',how='left')\n",
    "pwc_papers['year']=pd.to_datetime(pwc_papers['date']).dt.year\n",
    "annual_size=pwc_papers.groupby('year').size().reset_index()\n",
    "annual_size.columns=['year','pwc_size']\n",
    "inequity_years_df=pd.merge(inequity_years_df,annual_size,on='year',how='left')\n",
    "inequity_years_df['CV']=inequity_years_df['task'].apply(lambda x: in_category(x,'Computer Vision'))\n",
    "inequity_years_df['NLP']=inequity_years_df['task'].apply(lambda x: in_category(x,'Natural Language Processing'))\n",
    "inequity_years_df['Methodology']=inequity_years_df['task'].apply(lambda x: in_category(x,'Methodology'))\n",
    "inequity_years_df['Methodology']=inequity_years_df['Methodology'].apply(lambda x: 0 if x in Methodologies_to_Drop else x)\n",
    "inequity_years_df.to_csv(\"../Dataset_Curation/EntropyDatasetforRParentsOnly.txt\",sep='\\t',quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ddf2ac-a9a8-4ba6-af2f-c0ff01e7f28b",
   "metadata": {},
   "source": [
    "Optional dataset: Below is the entropy for all tasks, with no restrictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c84b9e97-5571-4069-a074-73c825bd7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parent_tasks=[i for i in parent_child_dict if len(parent_child_dict[i])!=0]\n",
    "task_ds_counts=entropy_dataset.groupby(['task','name']).size().reset_index()\n",
    "#task_ds_counts= task_ds_counts[task_ds_counts.task.isin(median_parent_tasks)]\n",
    "#task_ds_counts= task_ds_counts[task_ds_counts.task.isin(parent_tasks)]\n",
    "\n",
    "task_ds_counts.columns=['task', 'name','count']\n",
    "ginis=task_ds_counts.groupby(['task'])['count'].agg(corrected_gini).reset_index()\n",
    "ginis.columns=['task','gini']\n",
    "pielous=task_ds_counts.groupby(['task'])['count'].agg(pielou).reset_index()\n",
    "pielous.columns=['task','pielou']\n",
    "simpson=task_ds_counts.groupby(['task'])['count'].agg(alpha.simpson_e).reset_index()\n",
    "simpson.columns=['task','simpson']\n",
    "size=task_ds_counts.groupby(['task'])['count'].sum().reset_index()\n",
    "size.columns=['task','task_size']\n",
    "inequity_df=pd.merge(ginis,pielous,on=['task'])\n",
    "inequity_df=pd.merge(inequity_df,simpson,on=['task'],how='left')\n",
    "inequity_df=pd.merge(inequity_df,size,on=['task'],how='left')\n",
    "inequity_df['CV']=inequity_df['task'].apply(lambda x: in_category(x,'Computer Vision'))\n",
    "inequity_df['NLP']=inequity_df['task'].apply(lambda x: in_category(x,'Natural Language Processing'))\n",
    "inequity_df['Methodology']=inequity_df['task'].apply(lambda x: in_category(x,'Methodology'))\n",
    "inequity_df.to_csv(\"../Dataset_Curation/EntropyDatasetforRParentsandChildren.AllYears.txt\",sep='\\t',quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "70fe2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skbio.diversity import alpha\n",
    "task_ds_ycounts=entropy_dataset.groupby(['task','name',entropy_dataset.date]).size().reset_index()\n",
    "task_ds_ycounts.columns=['task', 'name','year','count']\n",
    "ginis=task_ds_ycounts.groupby(['task','year'])['count'].agg(corrected_gini).reset_index()\n",
    "ginis.columns=['task','year','gini']\n",
    "pielous=task_ds_ycounts.groupby(['task','year'])['count'].agg(pielou).reset_index()\n",
    "pielous.columns=['task','year','pielou']\n",
    "simpson=task_ds_ycounts.groupby(['task','year'])['count'].agg(alpha.simpson_e).reset_index()\n",
    "simpson.columns=['task','year','simpson']\n",
    "size=task_ds_ycounts.groupby(['task','year'])['count'].sum().reset_index()\n",
    "size.columns=['task','year','task_size']\n",
    "inequity_years_df=pd.merge(ginis,pielous,on=['task','year'])\n",
    "inequity_years_df=pd.merge(inequity_years_df,simpson,on=['task','year'],how='left')\n",
    "inequity_years_df=pd.merge(inequity_years_df,size,on=['task','year'],how='left')\n",
    "inequity_years_df=pd.merge(inequity_years_df,\n",
    "                           annual_data[['task','year','task_age','pwc_size','CV','NLP','Methodology']],\n",
    "                           on=['task','year'], how='left')\n",
    "inequity_years_df['CV']=inequity_years_df['task'].apply(lambda x: in_category(x,'Computer Vision'))\n",
    "inequity_years_df['NLP']=inequity_years_df['task'].apply(lambda x: in_category(x,'Natural Language Processing'))\n",
    "inequity_years_df['Methodology']=inequity_years_df['task'].apply(lambda x: in_category(x,'Methodology'))\n",
    "inequity_years_df['Methodology']=inequity_years_df['Methodology'].apply(lambda x: 0 if x in Methodologies_to_Drop else x)\n",
    "inequity_years_df.to_csv(\"../Dataset_Curation/EntropyDatasetforR.AllTasks.txt\",sep='\\t',quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d86ef-2927-4f62-b7af-678637849430",
   "metadata": {},
   "source": [
    "This dataset is only for the tasks that are larger than the median size and are parent tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "def2632c-971b-4e4c-a0b0-42f8dadc57db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks (parent-tasks only):  1203\n",
      "Number of datasets (parent-tasks only):  1933\n",
      "Number of papers (parent-tasks only):  26535\n"
     ]
    }
   ],
   "source": [
    "temp=entropy_dataset[entropy_dataset.task.isin(median_parent_tasks)]\n",
    "\n",
    "temp[['name','title']].drop_duplicates()\n",
    "\n",
    "datasets_w_authors=pd.read_csv(\"/mnt/c/Users/berna/Documents/GitHub/Life_of_a_Benchmark/Dataset_Curation/numdatasets.txt\")\n",
    "temp=entropy_dataset[entropy_dataset.name.isin(datasets_w_authors.Dataset_Name)]\n",
    "print(\"Number of tasks (parent-tasks only): \",temp.task.drop_duplicates().shape[0])\n",
    "print(\"Number of datasets (parent-tasks only): \",temp.name.drop_duplicates().shape[0])\n",
    "print(\"Number of papers (parent-tasks only): \",temp.title.drop_duplicates().shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab762dec-9a65-474a-ba3a-cdeccca72cd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac8276-1822-49eb-ba77-5c5c56a6926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIDs=pd.read_json('MAG_Linking_IDs.json')\n",
    "#MAGIDs=MAGIDs[['MAGID','PWC_Clean_Title','PWC_Title']]\n",
    "dataset_papers_MAG=pd.merge(MAGIDs,birth_papers.drop_duplicates(),left_on='PWC_Title',right_on='title')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77ab54-31aa-47fc-a569-67ac5c1d7979",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RQ 3 Affiliation Analysis\n",
    "\n",
    "The first file here was created for a part of the paper that didn't work out, but it contains Microsoft Academic Graph affiliations for all the dataset papers I could find. I use the last author affiliation although there seems to be similar levels of missingness in affiliation info with first and last author. Note that MAG will be offline at the end of 2021.\n",
    "\n",
    "This block merges these MAG affiliations with the \"entropy_dataset\" created for the Gini analyses in RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8f06209b-2cd4-424d-a6c1-60f84ff4f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_data=pd.read_csv('MAG_Affiliations.tsv',sep='\\t')\n",
    "entropy_dataset=pd.read_csv('../Dataset_Curation/EntropyDataset.txt')\n",
    "datasets_w_authors=mag_data[mag_data.Dataset_Introducing_Dataset==1][['Dataset_PWCFamilyId','Dataset_AffiliationId_Last',\n",
    "                                                              'Dataset_AffDisplayName_Last','Dataset_Name',\n",
    "                                                             'Dataset_AffLatitude_Last','Dataset_AffLongitude_Last']]\n",
    "datasets_w_authors=datasets_w_authors[~datasets_w_authors['Dataset_AffiliationId_Last'].isnull()].drop_duplicates()\n",
    "\n",
    "dataset_usages=pd.merge(datasets_w_authors,entropy_dataset,left_on='Dataset_Name', right_on='name').drop(['Unnamed: 0','task'],axis=1).drop_duplicates()\n",
    "dataset_usages=dataset_usages[~dataset_usages['date'].isnull()]\n",
    "dataset_usages['year']=dataset_usages['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac91f1ad-929d-485f-8ec0-066b1937bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_usages=pd.merge(datasets_w_authors,dataset_citing_papers,left_on='Dataset_Name', right_on='name').drop_duplicates()\n",
    "dataset_usages=dataset_usages[~dataset_usages['date'].isnull()]\n",
    "dataset_usages['year']=dataset_usages['date'].str[:4].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d918295b-c278-4f32-b2f9-2c26a70c47d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset_PWCFamilyId</th>\n",
       "      <th>Dataset_AffiliationId_Last</th>\n",
       "      <th>Dataset_AffDisplayName_Last</th>\n",
       "      <th>Dataset_Name</th>\n",
       "      <th>Dataset_AffLatitude_Last</th>\n",
       "      <th>Dataset_AffLongitude_Last</th>\n",
       "      <th>name</th>\n",
       "      <th>pwc_dataset_id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>is_problematic</th>\n",
       "      <th>paper_count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.108598e+09</td>\n",
       "      <td>20089843.0</td>\n",
       "      <td>Princeton University</td>\n",
       "      <td>ImageNet</td>\n",
       "      <td>40.348730</td>\n",
       "      <td>-74.659310</td>\n",
       "      <td>ImageNet</td>\n",
       "      <td>8</td>\n",
       "      <td>Temporal coherence-based self-supervised learn...</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>False</td>\n",
       "      <td>6081</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.108598e+09</td>\n",
       "      <td>20089843.0</td>\n",
       "      <td>Princeton University</td>\n",
       "      <td>ImageNet</td>\n",
       "      <td>40.348730</td>\n",
       "      <td>-74.659310</td>\n",
       "      <td>ImageNet</td>\n",
       "      <td>8</td>\n",
       "      <td>Closing the Generalization Gap of Adaptive Gra...</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>False</td>\n",
       "      <td>6081</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.108598e+09</td>\n",
       "      <td>20089843.0</td>\n",
       "      <td>Princeton University</td>\n",
       "      <td>ImageNet</td>\n",
       "      <td>40.348730</td>\n",
       "      <td>-74.659310</td>\n",
       "      <td>ImageNet</td>\n",
       "      <td>8</td>\n",
       "      <td>A Memory Network Approach for Story-based Temp...</td>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>False</td>\n",
       "      <td>6081</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.108598e+09</td>\n",
       "      <td>20089843.0</td>\n",
       "      <td>Princeton University</td>\n",
       "      <td>ImageNet</td>\n",
       "      <td>40.348730</td>\n",
       "      <td>-74.659310</td>\n",
       "      <td>ImageNet</td>\n",
       "      <td>8</td>\n",
       "      <td>Learning Asymmetric and Local Features in Mult...</td>\n",
       "      <td>2017-11-02</td>\n",
       "      <td>False</td>\n",
       "      <td>6081</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.108598e+09</td>\n",
       "      <td>20089843.0</td>\n",
       "      <td>Princeton University</td>\n",
       "      <td>ImageNet</td>\n",
       "      <td>40.348730</td>\n",
       "      <td>-74.659310</td>\n",
       "      <td>ImageNet</td>\n",
       "      <td>8</td>\n",
       "      <td>Scalable Methods for 8-bit Training of Neural ...</td>\n",
       "      <td>2018-05-25</td>\n",
       "      <td>False</td>\n",
       "      <td>6081</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78412</th>\n",
       "      <td>2.920975e+09</td>\n",
       "      <td>181369854.0</td>\n",
       "      <td>University of Erlangen-Nuremberg</td>\n",
       "      <td>100DOH</td>\n",
       "      <td>49.597916</td>\n",
       "      <td>11.004769</td>\n",
       "      <td>100DOH</td>\n",
       "      <td>5848</td>\n",
       "      <td>Revisiting Light Field Rendering with Deep Ant...</td>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>False</td>\n",
       "      <td>51</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78413</th>\n",
       "      <td>2.920975e+09</td>\n",
       "      <td>181369854.0</td>\n",
       "      <td>University of Erlangen-Nuremberg</td>\n",
       "      <td>100DOH</td>\n",
       "      <td>49.597916</td>\n",
       "      <td>11.004769</td>\n",
       "      <td>100DOH</td>\n",
       "      <td>5848</td>\n",
       "      <td>skweak: Weak Supervision Made Easy for NLP</td>\n",
       "      <td>2021-04-19</td>\n",
       "      <td>False</td>\n",
       "      <td>51</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78414</th>\n",
       "      <td>2.920975e+09</td>\n",
       "      <td>181369854.0</td>\n",
       "      <td>University of Erlangen-Nuremberg</td>\n",
       "      <td>100DOH</td>\n",
       "      <td>49.597916</td>\n",
       "      <td>11.004769</td>\n",
       "      <td>100DOH</td>\n",
       "      <td>5848</td>\n",
       "      <td>Predicting Aqueous Solubility of Organic Molec...</td>\n",
       "      <td>2021-05-26</td>\n",
       "      <td>False</td>\n",
       "      <td>51</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78415</th>\n",
       "      <td>2.920975e+09</td>\n",
       "      <td>181369854.0</td>\n",
       "      <td>University of Erlangen-Nuremberg</td>\n",
       "      <td>100DOH</td>\n",
       "      <td>49.597916</td>\n",
       "      <td>11.004769</td>\n",
       "      <td>100DOH</td>\n",
       "      <td>5848</td>\n",
       "      <td>Assessing epidemic curves for evidence of supe...</td>\n",
       "      <td>2021-06-22</td>\n",
       "      <td>False</td>\n",
       "      <td>51</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78416</th>\n",
       "      <td>2.920975e+09</td>\n",
       "      <td>181369854.0</td>\n",
       "      <td>University of Erlangen-Nuremberg</td>\n",
       "      <td>100DOH</td>\n",
       "      <td>49.597916</td>\n",
       "      <td>11.004769</td>\n",
       "      <td>100DOH</td>\n",
       "      <td>5848</td>\n",
       "      <td>Prediction of Hereditary Cancers Using Neural ...</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>False</td>\n",
       "      <td>51</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78408 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Dataset_PWCFamilyId  Dataset_AffiliationId_Last  \\\n",
       "0             2.108598e+09                  20089843.0   \n",
       "1             2.108598e+09                  20089843.0   \n",
       "2             2.108598e+09                  20089843.0   \n",
       "3             2.108598e+09                  20089843.0   \n",
       "4             2.108598e+09                  20089843.0   \n",
       "...                    ...                         ...   \n",
       "78412         2.920975e+09                 181369854.0   \n",
       "78413         2.920975e+09                 181369854.0   \n",
       "78414         2.920975e+09                 181369854.0   \n",
       "78415         2.920975e+09                 181369854.0   \n",
       "78416         2.920975e+09                 181369854.0   \n",
       "\n",
       "            Dataset_AffDisplayName_Last Dataset_Name  \\\n",
       "0                  Princeton University     ImageNet   \n",
       "1                  Princeton University     ImageNet   \n",
       "2                  Princeton University     ImageNet   \n",
       "3                  Princeton University     ImageNet   \n",
       "4                  Princeton University     ImageNet   \n",
       "...                                 ...          ...   \n",
       "78412  University of Erlangen-Nuremberg       100DOH   \n",
       "78413  University of Erlangen-Nuremberg       100DOH   \n",
       "78414  University of Erlangen-Nuremberg       100DOH   \n",
       "78415  University of Erlangen-Nuremberg       100DOH   \n",
       "78416  University of Erlangen-Nuremberg       100DOH   \n",
       "\n",
       "       Dataset_AffLatitude_Last  Dataset_AffLongitude_Last      name  \\\n",
       "0                     40.348730                 -74.659310  ImageNet   \n",
       "1                     40.348730                 -74.659310  ImageNet   \n",
       "2                     40.348730                 -74.659310  ImageNet   \n",
       "3                     40.348730                 -74.659310  ImageNet   \n",
       "4                     40.348730                 -74.659310  ImageNet   \n",
       "...                         ...                        ...       ...   \n",
       "78412                 49.597916                  11.004769    100DOH   \n",
       "78413                 49.597916                  11.004769    100DOH   \n",
       "78414                 49.597916                  11.004769    100DOH   \n",
       "78415                 49.597916                  11.004769    100DOH   \n",
       "78416                 49.597916                  11.004769    100DOH   \n",
       "\n",
       "       pwc_dataset_id                                              title  \\\n",
       "0                   8  Temporal coherence-based self-supervised learn...   \n",
       "1                   8  Closing the Generalization Gap of Adaptive Gra...   \n",
       "2                   8  A Memory Network Approach for Story-based Temp...   \n",
       "3                   8  Learning Asymmetric and Local Features in Mult...   \n",
       "4                   8  Scalable Methods for 8-bit Training of Neural ...   \n",
       "...               ...                                                ...   \n",
       "78412            5848  Revisiting Light Field Rendering with Deep Ant...   \n",
       "78413            5848         skweak: Weak Supervision Made Easy for NLP   \n",
       "78414            5848  Predicting Aqueous Solubility of Organic Molec...   \n",
       "78415            5848  Assessing epidemic curves for evidence of supe...   \n",
       "78416            5848  Prediction of Hereditary Cancers Using Neural ...   \n",
       "\n",
       "             date  is_problematic  paper_count  year  \n",
       "0      2018-06-18           False         6081  2018  \n",
       "1      2018-06-18           False         6081  2018  \n",
       "2      2018-05-08           False         6081  2018  \n",
       "3      2017-11-02           False         6081  2017  \n",
       "4      2018-05-25           False         6081  2018  \n",
       "...           ...             ...          ...   ...  \n",
       "78412  2021-04-14           False           51  2021  \n",
       "78413  2021-04-19           False           51  2021  \n",
       "78414  2021-05-26           False           51  2021  \n",
       "78415  2021-06-22           False           51  2021  \n",
       "78416  2021-06-25           False           51  2021  \n",
       "\n",
       "[78408 rows x 13 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_usages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40690ee0-1d5e-48b3-b204-d9a21d86320b",
   "metadata": {},
   "source": [
    "The next few blocks calculate Ginis across institutions and across datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96518cbd-7e3b-4c54-ab50-d655fb616cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block calculates % usages for each institution\n",
    "institutions=dataset_usages.groupby(['year','Dataset_AffiliationId_Last']).size().reset_index()\n",
    "institutions=institutions.rename({0:'count'},axis=1)\n",
    "institutions=institutions[(institutions.year>2010) & (institutions.year<2021)]\n",
    "institutions=institutions.sort_values(['year','count'],ascending=False)\n",
    "institutions['total']=institutions.groupby('year').transform(sum)['count']\n",
    "institutions['percent']=institutions['count']/institutions['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "051dc8e7-4f1b-4b38-8865-7f923929b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      year      gini\n",
      "1600  2011  0.312500\n",
      "1575  2012  0.714286\n",
      "1533  2013  0.662905\n",
      "1465  2014  0.701199\n",
      "1363  2015  0.755172\n",
      "1210  2016  0.774731\n",
      "1011  2017  0.800747\n",
      "749   2018  0.828733\n",
      "412   2019  0.853773\n",
      "0     2020  0.864382\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Now calculate Ginis by institution\n",
    "def gini(x):\n",
    "    if len(x)<2:return None\n",
    "    x=np.array(x)\n",
    "    \"\"\"Compute Gini coefficient of array of values\"\"\"\n",
    "    diffsum = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        diffsum += np.sum(np.abs(xi - x[i:]))\n",
    "    return diffsum / (len(x)**2 * np.mean(x))\n",
    "def corrected_gini(x):\n",
    "    if len(x)<2:return None\n",
    "    x=np.array(x)\n",
    "    \"\"\"Compute Gini coefficient of array of values\"\"\"\n",
    "    diffsum = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        diffsum += np.sum(np.abs(xi - x[i:]))\n",
    "    gini = diffsum / (len(x)**2 * np.mean(x))\n",
    "    return len(x)*gini/(len(x)-1) \n",
    "def pielou(x):\n",
    "    if len(x)<2:return None\n",
    "    x=np.array(x)\n",
    "    return entropy(x,base=2)/log(len(x),2)\n",
    "\n",
    "ginis=institutions.groupby(['year'])['count'].agg(corrected_gini)\n",
    "ginis.name='gini'\n",
    "institutions=pd.merge(institutions,ginis.reset_index(),on='year')\n",
    "institutions.to_csv('../Dataset_Curation/fullPWC_ginis_institutions.sensitivity.txt')\n",
    "print(institutions[['year','gini']].drop_duplicates().sort_values('year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31d4b0bf-9bfe-47c9-83cd-08494f66f71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      year      gini\n",
      "4750  2011  0.385714\n",
      "4720  2012  0.725320\n",
      "4653  2013  0.686115\n",
      "4530  2014  0.689168\n",
      "4321  2015  0.738886\n",
      "3969  2016  0.731193\n",
      "3431  2017  0.758397\n",
      "2659  2018  0.778676\n",
      "1520  2019  0.796661\n",
      "0     2020  0.800129\n"
     ]
    }
   ],
   "source": [
    "#Now calculate Ginis by datasets\n",
    "datasets=dataset_usages.groupby(['year','Dataset_Name']).size().reset_index()\n",
    "datasets=datasets.rename({0:'count'},axis=1)\n",
    "datasets=datasets[(datasets.year>2010) & (datasets.year<2021)]\n",
    "ginis=datasets.groupby(['year'])['count'].agg(corrected_gini)\n",
    "ginis.name='gini'\n",
    "datasets=datasets.sort_values(['year','count'],ascending=False)\n",
    "datasets['total']=datasets.groupby('year').transform(sum)['count']\n",
    "datasets['percent']=datasets['count']/datasets['total']\n",
    "datasets=pd.merge(datasets,ginis.reset_index(),on='year')\n",
    "print(datasets[['year','gini']].drop_duplicates().sort_values('year'))\n",
    "datasets.to_csv('../Dataset_Curation/fullPWC_ginis_datasets.sensitivity.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5801a0-cb62-4e0d-8f33-d1c86c58000e",
   "metadata": {},
   "source": [
    "The remaining code curates the data for the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6716cf24-96a6-4996-a83a-3108cec4d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First labele non academic institutions by excluding anything that is named college, univerisity, or institute\n",
    "institutions_map=dataset_usages.groupby(['Dataset_AffDisplayName_Last',\n",
    "                               'Dataset_AffLatitude_Last',\n",
    "                               'Dataset_AffLongitude_Last' ]).size().reset_index()\n",
    "institutions_map.rename({0:'usages'},inplace=True,axis=1)\n",
    "corporation=institutions_map[(institutions_map['Dataset_AffDisplayName_Last'].str.lower().str.find('institut')==-1) & \\\n",
    "                (institutions_map['Dataset_AffDisplayName_Last'].str.lower().str.find('universi')==-1) &\n",
    "                 (institutions_map['Dataset_AffDisplayName_Last'].str.lower().str.find('academ')==-1) &\\\n",
    "                (institutions_map['Dataset_AffDisplayName_Last'].str.lower().str.find('cole')==-1) &\\\n",
    "                (institutions_map['Dataset_AffDisplayName_Last'].str.lower().str.find('college')==-1) &\\\n",
    "                (institutions_map['Dataset_AffDisplayName_Last'].str.lower().str.find('college')==-1) ]\n",
    "\n",
    "#then I went through and fixed things that really weren't corporate\n",
    "non_industry=[\n",
    "'Air Force Research Laboratory',\n",
    "'Brigham and Women\\'s Hospital',\n",
    "'Centers for Disease Control and Prevention',\n",
    "'Cognition and Brain Sciences Unit',\n",
    "'CharitÃ©',\n",
    "'Commonwealth Scientific and Industrial Research Organisation',\n",
    "'DÃ©partement de MathÃ©matiques',\n",
    "'European Space Agency',\n",
    "'Foundation for Research & Technology â€“ Hellas',\n",
    "'German Aerospace Center',\n",
    "'German National Library of Science and Technology',\n",
    "'German Research Centre for Artificial Intelligence',\n",
    "'HEC MontrÃ©al',\n",
    "'KAIST',\n",
    "'Lawrence Berkeley National Laboratory',\n",
    "'Lawrence Livermore National Laboratory',\n",
    "'Max Planck Society',\n",
    "'Memorial Sloan Kettering Cancer Center',\n",
    "'National Center for Supercomputing Applications',\n",
    "'National Research Council',\n",
    "'Istituto Italiano di Tecnologia',\n",
    "'The Nature Conservancy',\n",
    "'United States Army Research Laboratory',\n",
    "'World Intellectual Property Organization',\n",
    "'fondazione bruno kessler',\n",
    "'Max Planck Society']\n",
    "\n",
    "corporation=corporation[~corporation.Dataset_AffDisplayName_Last.isin(non_industry)]\n",
    "\n",
    "\n",
    "institutions_map['corporation']=institutions_map['Dataset_AffDisplayName_Last'].apply(lambda x: x in corporation.Dataset_AffDisplayName_Last.values)\n",
    "institutions_map=institutions_map.sort_values('usages',ascending=False)\n",
    "institutions_map['cumpct']=institutions_map['usages'].cumsum()/institutions_map['usages'].sum()\n",
    "institutions_map.to_csv(\"../Dataset_Curation/map_data.sensitivity.tsv\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e39a52-9510-4287-a6f9-b7e829755310",
   "metadata": {},
   "source": [
    "What you really wanted to see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82615c2f-b6ab-455d-87ef-b40985c5f0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset_AffDisplayName_Last</th>\n",
       "      <th>Dataset_AffLatitude_Last</th>\n",
       "      <th>Dataset_AffLongitude_Last</th>\n",
       "      <th>usages</th>\n",
       "      <th>corporation</th>\n",
       "      <th>cumpct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Princeton University</td>\n",
       "      <td>40.348730</td>\n",
       "      <td>-74.659310</td>\n",
       "      <td>7007</td>\n",
       "      <td>False</td>\n",
       "      <td>0.100749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Stanford University</td>\n",
       "      <td>37.428230</td>\n",
       "      <td>-122.168861</td>\n",
       "      <td>6983</td>\n",
       "      <td>False</td>\n",
       "      <td>0.201153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>47.642094</td>\n",
       "      <td>-122.136986</td>\n",
       "      <td>5458</td>\n",
       "      <td>True</td>\n",
       "      <td>0.279630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT&amp;T</td>\n",
       "      <td>29.427322</td>\n",
       "      <td>-98.437510</td>\n",
       "      <td>4159</td>\n",
       "      <td>True</td>\n",
       "      <td>0.339430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Max Planck Society</td>\n",
       "      <td>48.141170</td>\n",
       "      <td>11.581836</td>\n",
       "      <td>2779</td>\n",
       "      <td>False</td>\n",
       "      <td>0.379387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>The Chinese University of Hong Kong</td>\n",
       "      <td>22.419720</td>\n",
       "      <td>114.206795</td>\n",
       "      <td>2583</td>\n",
       "      <td>False</td>\n",
       "      <td>0.416526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Google</td>\n",
       "      <td>37.422054</td>\n",
       "      <td>-122.084442</td>\n",
       "      <td>2307</td>\n",
       "      <td>True</td>\n",
       "      <td>0.449697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>New York University</td>\n",
       "      <td>40.730000</td>\n",
       "      <td>-73.995000</td>\n",
       "      <td>2140</td>\n",
       "      <td>False</td>\n",
       "      <td>0.480467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Toyota Technological Institute at Chicago</td>\n",
       "      <td>41.784363</td>\n",
       "      <td>-87.593320</td>\n",
       "      <td>1610</td>\n",
       "      <td>False</td>\n",
       "      <td>0.503616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Dataset_AffDisplayName_Last  Dataset_AffLatitude_Last  \\\n",
       "242                       Princeton University                 40.348730   \n",
       "284                        Stanford University                 37.428230   \n",
       "185                                  Microsoft                 47.642094   \n",
       "0                                         AT&T                 29.427322   \n",
       "178                         Max Planck Society                 48.141170   \n",
       "301        The Chinese University of Hong Kong                 22.419720   \n",
       "97                                      Google                 37.422054   \n",
       "214                        New York University                 40.730000   \n",
       "309  Toyota Technological Institute at Chicago                 41.784363   \n",
       "\n",
       "     Dataset_AffLongitude_Last  usages  corporation    cumpct  \n",
       "242                 -74.659310    7007        False  0.100749  \n",
       "284                -122.168861    6983        False  0.201153  \n",
       "185                -122.136986    5458         True  0.279630  \n",
       "0                   -98.437510    4159         True  0.339430  \n",
       "178                  11.581836    2779        False  0.379387  \n",
       "301                 114.206795    2583        False  0.416526  \n",
       "97                 -122.084442    2307         True  0.449697  \n",
       "214                 -73.995000    2140        False  0.480467  \n",
       "309                 -87.593320    1610        False  0.503616  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "institutions_map[institutions_map['cumpct']<=.51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "119553c4-40da-4620-929e-58ac41ea24ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-7eab03ab2941>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_papers['date']=None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'source_dest_edgelist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7eab03ab2941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset_papers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets_pwc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset_papers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdest_papers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_dest_edgelist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdest_papers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbirth_papers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbirth_edgelist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_dest_edgelist' is not defined"
     ]
    }
   ],
   "source": [
    "#source_dest_edgelist=pd.read_csv('source_dest_edgelist.csv')\n",
    "#homegrown_edgelist=pd.read_csv('source_dest_edgelist.csv')\n",
    "#birth_edgelist=pd.read_csv('birth_edgelist.csv')\n",
    "dataset_papers=datasets_pwc[['name','title']]\n",
    "dataset_papers['date']=None\n",
    "dest_papers=source_dest_edgelist[['name','title','date']]\n",
    "dest_papers.columns=['name','title','date']\n",
    "birth_papers=birth_edgelist[['name','title','date']]\n",
    "homegrown_papers=homegrown_edgelist[['name','title','date']]\n",
    "full_dataset=pd.concat([dataset_papers,dest_papers,birth_papers,homegrown_papers]).drop_duplicates()\n",
    "print(\"Number of datasets born and then used at least once: \",birth_edgelist['name'].drop_duplicates().shape[0])\n",
    "print(\"Total number of unique usages within introducing paper task: \", homegrown_edgelist[['name','title']].drop_duplicates().shape[0])\n",
    "print(\"Total number of unique usages from outside paper introducing task: \", source_dest_edgelist[['name','title']].drop_duplicates().shape[0])\n",
    "print(\"Total number of dataset-usages: \", full_dataset[~full_dataset.date.isnull()][['name','title']].drop_duplicates().shape[0])\n",
    "print(\"Total number of dataset-using papers: \", full_dataset[~full_dataset.date.isnull()][['title']].drop_duplicates().shape[0])\n",
    "print(\"Total number of datasets: \", full_dataset[['name']].drop_duplicates().shape[0])\n",
    "print(\"Total number of tasks involved: \",pd.concat([birth_edgelist['task'],homegrown_edgelist['task'],\n",
    "           source_dest_edgelist['source_task'],source_dest_edgelist['dest_task']]).drop_duplicates().shape[0])\n",
    "full_dataset.to_csv('./PWC_Data/Derivative_Datasets/ValidPaperDataset-Titles.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdce313-26a3-4aec-a386-2e7b01e2546f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Figure 4: Pie charts for Figure 4\n",
    "\n",
    "The following blocks create the pie charts for figure 4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7672056-9e91-4f09-a888-26cc3b8decd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p='Image Generation'\n",
    "datasets_borrow=source_dest_edgelist[source_dest_edgelist.dest_task==p].drop_duplicates(['name','title']).groupby('name').size().sort_values().to_frame()\n",
    "datasets_borrow['source']='blue'\n",
    "datasets_homegrown=homegrown_edgelist[homegrown_edgelist.task==p].drop_duplicates(['name','title']).groupby('name').size().sort_values().to_frame()\n",
    "datasets_homegrown['source']='orange'\n",
    "dataset_df=pd.concat([ datasets_borrow,datasets_homegrown]).reset_index().sort_values(0,ascending=False)\n",
    "dataset_df=dataset_df.rename({0:'count'},axis=1)\n",
    "\n",
    "tasks_borrow=source_dest_edgelist[source_dest_edgelist.dest_task==p].drop_duplicates(['name','title']).groupby('source_task').size().sort_values().to_frame()\n",
    "tasks_borrow['source']='blue'\n",
    "tasks_homegrown=homegrown_edgelist[homegrown_edgelist.task==p].drop_duplicates(['name','title']).groupby('task').size().sort_values().to_frame()\n",
    "tasks_homegrown['source']='orange'\n",
    "tasks_df=pd.concat([ tasks_borrow,tasks_homegrown]).reset_index().sort_values(0,ascending=False)\n",
    "tasks_df=tasks_df.rename({0:'count','index':'task'},axis=1)\n",
    "\n",
    "dataset_df['cumulative']=dataset_df.sort_values('count',ascending=False)['count'].cumsum()/dataset_df.sort_values('count',ascending=False)['count'].sum()\n",
    "#tasks_df.set_index('task').plot.pie(y='count')\n",
    "other_count=dataset_df[dataset_df['cumulative']>.90]['count'].sum()\n",
    "dataset_df=dataset_df[dataset_df['cumulative']<.90]\n",
    "dataset_df=dataset_df.append({'name':'Other','count':other_count,'cumulative':1,'source':'gray'},ignore_index=True)\n",
    "\n",
    "tasks_df['cumulative']=tasks_df.sort_values('count',ascending=False)['count'].cumsum()/tasks_df.sort_values('count',ascending=False)['count'].sum()\n",
    "#tasks_df.set_index('task').plot.pie(y='count')\n",
    "other_count=tasks_df[tasks_df['cumulative']>.90]['count'].sum()\n",
    "tasks_df=tasks_df[tasks_df['cumulative']<.90]\n",
    "tasks_df=tasks_df.append({'task':'Other','count':other_count,'cumulative':1,'source':'gray'},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8ed91-8489-45be-8103-060a71630d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "fig = go.Figure(data=[go.Pie(labels=tasks_df['task'], \n",
    "                             values=tasks_df['count'],\n",
    "                            pull=[0, 0, 0, 0.0,0.2,0,0])])\n",
    "fig.update_layout(\n",
    "    font_family=\"Arial\",\n",
    "    title_font_family=\"Arial\",\n",
    "    font_color='black',\n",
    ")\n",
    "fig.update_traces(marker=dict(colors=px.colors.qualitative.Set1,line=dict(color='#000000', width=1)),textfont_color='black')\n",
    "fig.show()\n",
    "fig.write_image(\"/mnt/c/Users/berna/Documents/GoogleDataProject/ImportPlots/ImageGenerationTasks.svg\")\n",
    "fig = go.Figure(data=[go.Pie(labels=dataset_df['name'], \n",
    "                             values=dataset_df['count'],\n",
    "                            pull=[0, 0, 0, 0.0,0,0.2,0])])\n",
    "fig.update_layout(\n",
    "    font_family=\"Arial\",\n",
    "    title_font_family=\"Arial\",\n",
    "    font_color='black',\n",
    ")\n",
    "fig.update_traces(marker=dict(colors=px.colors.qualitative.Set3,line=dict(color='#000000', width=1)),textfont_color='black')\n",
    "#fig.write_image(\"ImageGenDatasets.svg\")\n",
    "fig.show()\n",
    "fig.write_image(\"/mnt/c/Users/berna/Documents/GoogleDataProject/ImportPlots/ImageGenerationDatasets.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306c978-39a7-4a9a-a68f-6123dfcfd149",
   "metadata": {},
   "outputs": [],
   "source": [
    "p='Face Recognition'\n",
    "datasets_borrow=source_dest_edgelist[source_dest_edgelist.dest_task==p].drop_duplicates(['name','title']).groupby('name').size().sort_values().to_frame()\n",
    "datasets_borrow['source']='blue'\n",
    "datasets_homegrown=homegrown_edgelist[homegrown_edgelist.task==p].drop_duplicates(['name','title']).groupby('name').size().sort_values().to_frame()\n",
    "datasets_homegrown['source']='orange'\n",
    "dataset_df=pd.concat([ datasets_borrow,datasets_homegrown]).reset_index().sort_values(0,ascending=False)\n",
    "dataset_df=dataset_df.rename({0:'count'},axis=1)\n",
    "\n",
    "tasks_borrow=source_dest_edgelist[source_dest_edgelist.dest_task==p].drop_duplicates(['name','title']).groupby('source_task').size().sort_values().to_frame()\n",
    "tasks_borrow['source']='blue'\n",
    "tasks_homegrown=homegrown_edgelist[homegrown_edgelist.task==p].drop_duplicates(['name','title']).groupby('task').size().sort_values().to_frame()\n",
    "tasks_homegrown['source']='orange'\n",
    "tasks_df=pd.concat([ tasks_borrow,tasks_homegrown]).reset_index().sort_values(0,ascending=False)\n",
    "tasks_df=tasks_df.rename({0:'count','index':'task'},axis=1)\n",
    "\n",
    "dataset_df['cumulative']=dataset_df.sort_values('count',ascending=False)['count'].cumsum()/dataset_df.sort_values('count',ascending=False)['count'].sum()\n",
    "#tasks_df.set_index('task').plot.pie(y='count')\n",
    "other_count=dataset_df[dataset_df['cumulative']>.85]['count'].sum()\n",
    "dataset_df=dataset_df[dataset_df['cumulative']<.85]\n",
    "dataset_df=dataset_df.append({'name':'Other','count':other_count,'cumulative':1,'source':'gray'},ignore_index=True)\n",
    "\n",
    "tasks_df['cumulative']=tasks_df.sort_values('count',ascending=False)['count'].cumsum()/tasks_df.sort_values('count',ascending=False)['count'].sum()\n",
    "#tasks_df.set_index('task').plot.pie(y='count')\n",
    "other_count=tasks_df[tasks_df['cumulative']>.85]['count'].sum()\n",
    "tasks_df=tasks_df[tasks_df['cumulative']<.85]\n",
    "tasks_df=tasks_df.append({'task':'Other','count':other_count,'cumulative':1,'source':'gray'},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ece0fb-18d4-42f0-b443-9d3f6782498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=tasks_df['task'], \n",
    "                             values=tasks_df['count'],\n",
    "                            pull=[0, 0, 0, 0.2,0,0,0])])\n",
    "fig.update_layout(\n",
    "    font_family=\"Arial\",\n",
    "    title_font_family=\"Arial\",\n",
    "    font_color='black',\n",
    ")\n",
    "fig.update_traces(marker=dict(colors=px.colors.qualitative.Set1,line=dict(color='#000000', width=1)),textfont_color='black')\n",
    "fig.show()\n",
    "temp=[i for i in px.colors.qualitative.Set1]\n",
    "temp[3]='rgb(255,255,,255)'\n",
    "temp[4]='rgb(255,255,255)'\n",
    "temp\n",
    "fig = go.Figure(data=[go.Pie(labels=dataset_df['name'], \n",
    "                             values=dataset_df['count'],\n",
    "                            pull=[0, 0, 0,.2,.2,0,0])])\n",
    "fig.update_layout(\n",
    "    font_family=\"Arial\",\n",
    "    title_font_family=\"Arial\",\n",
    "    font_color='black',\n",
    ")\n",
    "fig.update_traces(marker=dict(colors=temp,line=dict(color='#000000', width=1)),textfont_color='black')\n",
    "fig.show()\n",
    "fig.write_image(\"/mnt/c/Users/berna/Documents/GoogleDataProject/ImportPlots/FaceRecognitionDatasets.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e7c9e4-8e4b-4fb0-af27-4e10958a2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tasks_df['cumulative']=tasks_df.sort_values('count',ascending=False)['count'].cumsum()/tasks_df.sort_values('count',ascending=False)['count'].sum()\n",
    "#tasks_df.set_index('task').plot.pie(y='count')\n",
    "other_count=tasks_df[tasks_df['cumulative']>.99]['count'].sum()\n",
    "tasks_df=tasks_df[tasks_df['cumulative']<.99]\n",
    "tasks_df=tasks_df.append({'task':'Other','count':other_count,'cumulative':1,'source':'gray'},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd31219-fd7d-4142-bf9b-90f7c96604e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['cumulative']=dataset_df.sort_values('count',ascending=False)['count'].cumsum()/dataset_df.sort_values('count',ascending=False)['count'].sum()\n",
    "#tasks_df.set_index('task').plot.pie(y='count')\n",
    "other_count=dataset_df[dataset_df['cumulative']>.99]['count'].sum()\n",
    "dataset_df=dataset_df[dataset_df['cumulative']<.99]\n",
    "dataset_df=dataset_df.append({'name':'Other','count':other_count,'cumulative':1,'source':'gray'},ignore_index=True)\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378afb9-5ccf-4896-91fe-0323b6b788c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=tasks_df['task'], \n",
    "                             values=tasks_df['count'],\n",
    "                            pull=[0, 0, 0, 0.2,0,0,0])])\n",
    "fig.update_layout(\n",
    "    font_family=\"Arial\",\n",
    "    title_font_family=\"Arial\",\n",
    "    font_color='black',\n",
    ")\n",
    "fig.update_traces(marker=dict(colors=px.colors.qualitative.Set1,line=dict(color='#000000', width=1)),textfont_color='black')\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=dataset_df['name'], \n",
    "                             values=dataset_df['count'],\n",
    "                            pull=[0, 0, 0, 0.2,0,0,0])])\n",
    "fig.update_layout(\n",
    "    font_family=\"Arial\",\n",
    "    title_font_family=\"Arial\",\n",
    "    font_color='black',\n",
    ")\n",
    "fig.update_traces(marker=dict(colors=px.colors.qualitative.Set1,line=dict(color='#000000', width=1)),textfont_color='black')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0afb9-0cd3-4bf6-90e9-d56d09a6ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "inequity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33f0aa-bdc0-4a81-aa43-e58598928f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ds_ycounts[(task_ds_ycounts.year==2015) & (task_ds_ycounts.task=='3D Human Pose Estimation')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a5118-6c01-4a3d-b506-5b4ef90226b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e52b74ed-b932-4659-ad42-ae9ccfa1511b",
   "metadata": {},
   "source": [
    "## Appendix Figures 6 and 7\n",
    "These two figures appear in the appendix as summaries of the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39a68d20-d9d5-433a-8203-5cd1db8bc90f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0ac4413441e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset_usage_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hist'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Truncated Distribution of Dataset Usages'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_usage_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'truncated_dist.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "source_dest_edgelist=pd.read_csv('./PWC_Data/Derivative_Datasets/source_dest_edgelist.csv')\n",
    "homegrown_edgelist=pd.read_csv('./PWC_Data/Derivative_Datasets/homegrown_edgelist')\n",
    "birth_edgelist=pd.read_csv('./PWC_Data/Derivative_Datasets/birth_edgelist.csv')\n",
    "dataset_papers=datasets[['name','title']]\n",
    "dataset_papers['date']=None\n",
    "dest_papers=source_dest_edgelist[['name','title','date']]\n",
    "dest_papers.columns=['name','title','date']\n",
    "birth_papers=birth_edgelist[['name','title','date']]\n",
    "homegrown_papers=homegrown_edgelist[['name','title','date']]\n",
    "full_dataset=pd.concat([dataset_papers,dest_papers,birth_papers,homegrown_papers]).drop_duplicates()\n",
    "temp=full_dataset.groupby('name').size().sort_values()\n",
    "dataset_usage_dist=temp[(temp>5)&(temp<500)].plot(kind='hist',bins=500,figsize=[8,4],title='Truncated Distribution of Dataset Usages')\n",
    "fig = dataset_usage_dist.get_figure()\n",
    "fig.savefig('../Appendix/TruncatedDatasetsByUsages.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7d7d0-32ae-4760-a9ba-f773d8cb639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_size=pwc_papers.groupby('year').size().reset_index()\n",
    "temp=annual_size.plot(figsize=[8,4],x='year',y=0,title='PWC Corpus papers per year',xlim=[2009,2020],legend=False)\n",
    "fig = temp.get_figure()\n",
    "fig.savefig('../Appendix/PWCSizebyYear.png')\n",
    "\n",
    "pwc_papers.title.drop_duplicates().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
